{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcoder.treeofcompletions.PriorityQueue import PriorityQueue, Conversation\n",
    "import openai\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\"\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Please provide a cute easy code in python \"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a simple Python code snippet that prints a cute message:\n",
      "\n",
      "```python\n",
      "print(\"Hello, cutie!\")\n",
      "```\n",
      "\n",
      "This code will display the message \"Hello, cutie!\" when executed. Feel free to customize the message to make it even cuter!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from llmcoder.utils import get_openai_key\n",
    "\n",
    "client = OpenAI(api_key = get_openai_key())\n",
    "completion = client.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\", temperature=0.7, n = 3)\n",
    "print(completion.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a cute and easy Python code that prints a heart shape:\n",
      "\n",
      "```python\n",
      "heart = '''\n",
      "  ❤️ ❤️\n",
      "❤️ ❤️ ❤️\n",
      "  ❤️ ❤️\n",
      "    ❤️\n",
      "'''\n",
      "\n",
      "print(heart)\n",
      "```\n",
      "\n",
      "When you run this code, it will print a heart shape using the heart emoji ❤️. Enjoy!\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[1].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.2115587]\n"
     ]
    }
   ],
   "source": [
    "from llmcoder.analyze.GPTScoreAnalyzer import GPTScoreAnalyzer\n",
    "analyzer = GPTScoreAnalyzer()\n",
    "score1 = analyzer.score_code(completion.choices[1].message.content)\n",
    "print(score1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": completion.choices[1].message.content,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = Conversation(score1, messages=messages, analyzer_results_history=[])\n",
    "conversation._add_message(new_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcoder.LLMCoder import LLMCoder\n",
    "llmcoder = LLMCoder(\n",
    "    analyzers=[\"mypy_analyzer_v1\"],\n",
    "    max_iter=3,\n",
    "    feedback_variant=\"separate\",\n",
    "    n_procs=4, \n",
    "    log_conversation=False, \n",
    "    verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcoder.analyze.factory import AnalyzerFactory\n",
    "analyzers = {\n",
    "    analyzer: AnalyzerFactory.create_analyzer(\"mypy_analyzer_v1\", verbose=True) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation._get_messages()[1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llmcoder._add_message_to_conversation(\"assistant\", conversation, message = new_message, model= \"gpt-3.5-turbo\", n = 3)\n",
    "# llmcoder._get_completions_for(conversation,  model= \"gpt-3.5-turbo\", temperature = 0.7, n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMcoder] Creating first completions...\n",
      "There are no analyzer results\n",
      "Old messages of the conversation: [{'role': 'system', 'content': \"You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\\nYou are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\\nPrecisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\\nYour code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\\nYour unique capability is to provide completions without altering, repeating, or commenting on the original code.\\nYou offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\\nThis approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\\nYour response begins with the next characters of the line if the last line of the user's code is incomplete, or the next line if the last line of the user's code is complete.\\nYour application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\"}]\n",
      "New messages of the conversation: [{'role': 'system', 'content': \"You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\\nYou are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\\nPrecisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\\nYour code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\\nYour unique capability is to provide completions without altering, repeating, or commenting on the original code.\\nYou offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\\nThis approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\\nYour response begins with the next characters of the line if the last line of the user's code is incomplete, or the next line if the last line of the user's code is complete.\\nYour application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\"}, {'role': 'user', 'content': 'Please provide a cute easy code in python '}]\n",
      "[LLMcoder] Analyzing 3 completions...\n",
      "[LLMcoder] Analyzing code in separate mode...\n",
      "[LLMcoder] Running mypy_analyzer_v1...\n",
      "[LLMcoder] Analyzing code in separate mode...\n",
      "[LLMcoder] Running mypy_analyzer_v1...\n",
      "[LLMcoder] Analyzing code in separate mode...\n",
      "[LLMcoder] Running mypy_analyzer_v1...\n",
      "[Mypy] No missing stubs found.\n",
      "Found line: C:\\Users\\anaca\\AppData\\Local\\Temp\\tmppkugpvlm.py:1: error: unterminated string literal (detected at line 1)  [syntax]\n",
      "[Mypy] C:\\Users\\anaca\\AppData\\Local\\Temp\\tmppkugpvlm.py:1: error: unterminated string literal (detected at line 1)  [syntax]\n",
      "Found line: Found 1 error in 1 file (errors prevented further checking)\n",
      "[Mypy] Found 1 error in 1 file (errors prevented further checking)\n",
      "Found line: \n",
      "[Mypy] No missing stubs found.\n",
      "Found line: C:\\Users\\anaca\\AppData\\Local\\Temp\\tmpwzm5qi5o.py:1: error: unterminated string literal (detected at line 1)  [syntax]\n",
      "[Mypy] C:\\Users\\anaca\\AppData\\Local\\Temp\\tmpwzm5qi5o.py:1: error: unterminated string literal (detected at line 1)  [syntax]\n",
      "Found line: Found 1 error in 1 file (errors prevented further checking)\n",
      "[Mypy] Found 1 error in 1 file (errors prevented further checking)\n",
      "Found line: \n",
      "[LLMcoder] An exception occurred during analysis of choice: invalid literal for int() with base 10: '\\\\Users\\\\anaca\\\\AppData\\\\Local\\\\Temp\\\\tmpwzm5qi5o.py'\n",
      "[Mypy] No missing stubs found.\n",
      "Found line: C:\\Users\\anaca\\AppData\\Local\\Temp\\tmpzkpwcj57.py:1: error: unterminated string literal (detected at line 1)  [syntax]\n",
      "[Mypy] C:\\Users\\anaca\\AppData\\Local\\Temp\\tmpzkpwcj57.py:1: error: unterminated string literal (detected at line 1)  [syntax]\n",
      "Found line: Found 1 error in 1 file (errors prevented further checking)\n",
      "[Mypy] Found 1 error in 1 file (errors prevented further checking)\n",
      "Found line: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '\\\\Users\\\\anaca\\\\AppData\\\\Local\\\\Temp\\\\tmpwzm5qi5o.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llmcoder\u001b[38;5;241m.\u001b[39mcomplete(conversation\u001b[38;5;241m.\u001b[39m_get_messages()[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m], temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\quinto.curso\\ai.tools\\llmcoder\\23ws-LLMcoder\\src\\llmcoder\\LLMCoder.py:176\u001b[0m, in \u001b[0;36mLLMCoder.complete\u001b[1;34m(self, code, temperature, n)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LLMcoder] Creating first completions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 176\u001b[0m conversation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(code, temperature, n)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conversation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompletion generation failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\quinto.curso\\ai.tools\\llmcoder\\23ws-LLMcoder\\src\\llmcoder\\LLMCoder.py:526\u001b[0m, in \u001b[0;36mLLMCoder.step\u001b[1;34m(self, code, temperature, n)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew messages of the conversation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversation\u001b[38;5;241m.\u001b[39m_get_messages()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;66;03m# Get 3 completions from the assistant and add them to the priority queue\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_message_to_conversation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, conversation, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_feedback, temperature\u001b[38;5;241m=\u001b[39mtemperature, n\u001b[38;5;241m=\u001b[39mn)  \u001b[38;5;66;03m# model_first works quite good here\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;66;03m# Check if the completions passed\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\quinto.curso\\ai.tools\\llmcoder\\23ws-LLMcoder\\src\\llmcoder\\LLMCoder.py:389\u001b[0m, in \u001b[0;36mLLMCoder._add_message_to_conversation\u001b[1;34m(self, role, conversation, message, model, temperature, n)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;66;03m# If the user is the assistant, generate a response\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m role \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 389\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_completions_for(conversation, model, temperature, n)\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;66;03m# If the completion generation did not work, abort\u001b[39;00m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Documents\\quinto.curso\\ai.tools\\llmcoder\\23ws-LLMcoder\\src\\llmcoder\\LLMCoder.py:321\u001b[0m, in \u001b[0;36mLLMCoder._get_completions_for\u001b[1;34m(self, conversation, model, temperature, n)\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    320\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LLMcoder] An exception occurred during analysis of choice: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 321\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    322\u001b[0m             \u001b[38;5;66;03m# analysis_results_list.append({\u001b[39;00m\n\u001b[0;32m    323\u001b[0m             \u001b[38;5;66;03m#     \"score\": -np.inf,\u001b[39;00m\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;66;03m#     \"type\": \"ignore\",\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    328\u001b[0m \n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Select the best completion\u001b[39;00m\n\u001b[0;32m    330\u001b[0m best_conversation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversations\u001b[38;5;241m.\u001b[39mget_highest_scored_conversation()\n",
      "File \u001b[1;32m~\\Documents\\quinto.curso\\ai.tools\\llmcoder\\23ws-LLMcoder\\src\\llmcoder\\LLMCoder.py:302\u001b[0m, in \u001b[0;36mLLMCoder._get_completions_for\u001b[1;34m(self, conversation, model, temperature, n)\u001b[0m\n\u001b[0;32m    299\u001b[0m future \u001b[38;5;241m=\u001b[39m choice_to_future[i]\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;66;03m# Update the analyzer results history with the results of total completion\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m     analysis_results \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe analysis results are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalysis_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    304\u001b[0m     analysis_results_list\u001b[38;5;241m.\u001b[39mappend(analysis_results)\n",
      "File \u001b[1;32mc:\\Users\\anaca\\miniconda3\\envs\\llmcoder_env\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\anaca\\miniconda3\\envs\\llmcoder_env\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anaca\\miniconda3\\envs\\llmcoder_env\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32m~\\Documents\\quinto.curso\\ai.tools\\llmcoder\\23ws-LLMcoder\\src\\llmcoder\\LLMCoder.py:457\u001b[0m, in \u001b[0;36mLLMCoder.run_analyzers\u001b[1;34m(self, code, completion)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    456\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LLMcoder] Running \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalyzer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 457\u001b[0m         analyzer_results[analyzer_name] \u001b[38;5;241m=\u001b[39m analyzer_instance\u001b[38;5;241m.\u001b[39manalyze(code, completion)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;66;03m# In coworker mode, the analyzers are run in parallel and share a context\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeedback_variant \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoworker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\quinto.curso\\ai.tools\\llmcoder\\23ws-LLMcoder\\src\\llmcoder\\analyze\\MypyAnalyzer.py:113\u001b[0m, in \u001b[0;36mMypyAnalyzer.analyze\u001b[1;34m(self, input, completion, install_stubs, mypy_args, context)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(temp_file_name):\n\u001b[1;32m--> 113\u001b[0m         line_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m line_number \u001b[38;5;241m>\u001b[39m n_input_lines:\n\u001b[0;32m    115\u001b[0m             filtered_result\u001b[38;5;241m.\u001b[39mappend(line)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '\\\\Users\\\\anaca\\\\AppData\\\\Local\\\\Temp\\\\tmpwzm5qi5o.py'"
     ]
    }
   ],
   "source": [
    "llmcoder.complete(conversation._get_messages()[1][\"content\"], temperature=0.7, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " I calculated the candidates\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "candidates = llmcoder.client.chat.completions.create(messages=conversation._get_messages(), model=model, temperature=0.7, n=3)  # type: ignore\n",
    "\n",
    "print(\"\\n I calculated the candidates\")\n",
    "# Filter out completions that are repetitions of previous mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llmcoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m llmcoder\u001b[38;5;241m.\u001b[39m_get_completions_for(conversation, temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llmcoder' is not defined"
     ]
    }
   ],
   "source": [
    "results = llmcoder._get_completions_for(conversation, temperature = 0.7, n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
