{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcoder.treeofcompletions.PriorityQueue import PriorityQueue, Conversation\n",
    "import openai\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\"\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \" Please provide a cute easy code in python \"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a simple code snippet that prints a cute message in Python:\n",
      "\n",
      "```python\n",
      "print(\"You are as cute as a button!\")\n",
      "```\n",
      "\n",
      "Feel free to customize the message to make it even cuter!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from llmcoder.utils import get_openai_key\n",
    "\n",
    "client = OpenAI(api_key = get_openai_key())\n",
    "completion = client.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\", temperature=0.7, n = 3)\n",
    "print(completion.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a cute and easy Python code that prints a heart shape:\n",
      "\n",
      "```python\n",
      "heart = [\n",
      "    \"  ❤️ ❤️  \",\n",
      "    \" ❤️   ❤️ \",\n",
      "    \"❤️     ❤️\",\n",
      "    \" ❤️   ❤️ \",\n",
      "    \"  ❤️ ❤️  \"\n",
      "]\n",
      "\n",
      "for line in heart:\n",
      "    print(line)\n",
      "```\n",
      "\n",
      "When you run this code, it will output a heart shape made of hearts!\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[1].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.2115587]\n"
     ]
    }
   ],
   "source": [
    "from llmcoder.analyze.GPTScoreAnalyzer import GPTScoreAnalyzer\n",
    "analyzer = GPTScoreAnalyzer()\n",
    "score1 = analyzer.score_code(completion.choices[1].message.content)\n",
    "print(score1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": completion.choices[1].message.content,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = Conversation(score1, messages=messages, analyzer_results_history=[])\n",
    "conversation._add_message(new_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcoder.LLMCoder import LLMCoder\n",
    "llmcoder = LLMCoder(\n",
    "    analyzers=[\"signature_analyzer_v1\", \"gpt_score_analyzer_v1\"],\n",
    "    max_iter=3,\n",
    "    feedback_variant=\"coworker\",\n",
    "    n_procs=4, \n",
    "    log_conversation=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation._get_messages()[1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llmcoder._add_message_to_conversation(\"assistant\", conversation, message = new_message, model= \"gpt-3.5-turbo\", n = 3)\n",
    "# llmcoder._get_completions_for(conversation,  model= \"gpt-3.5-turbo\", temperature = 0.7, n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMcoder] Creating first completions...\n",
      "There are no analyzer results\n",
      "Old messages of the conversation: [{'role': 'system', 'content': \"You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\\nYou are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\\nPrecisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\\nYour code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\\nYour unique capability is to provide completions without altering, repeating, or commenting on the original code.\\nYou offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\\nThis approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\\nYour response begins with the next characters of the line if the last line of the user's code is incomplete, or the next line if the last line of the user's code is complete.\\nYour application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\"}]\n",
      "New messages of the conversation: [{'role': 'system', 'content': \"You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\\nYou are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\\nPrecisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\\nYour code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\\nYour unique capability is to provide completions without altering, repeating, or commenting on the original code.\\nYou offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\\nThis approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\\nYour response begins with the next characters of the line if the last line of the user's code is incomplete, or the next line if the last line of the user's code is complete.\\nYour application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\"}, {'role': 'user', 'content': ' Please provide a cute easy code in python '}]\n",
      "\n",
      " I calculated the candidates\n",
      "[LLMcoder] Analyzing 2 completions...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "Entering Thread 0\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "After analyzing, the results were dict_values([True, 'info', 0, 'All functions and classes in your completion are called correctly (their signatures match with the documentation).'])\n",
      "[LLMcoder] Running gpt_score_analyzer_v1...\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "After analyzing, the results were dict_values([True, 'info', 0, 'All functions and classes in your completion are called correctly (their signatures match with the documentation).'])\n",
      "[LLMcoder] Running gpt_score_analyzer_v1...\n",
      "After analyzing, the results were dict_values(['score', 8.909126743510708, True, ''])\n",
      "Generated completion 0 with score: 0\n",
      "Entering Thread 1\n",
      "After analyzing, the results were dict_values(['score', 9.211558703193814, True, ''])\n",
      "Generated completion 1 with score: 1\n",
      "Error. The heap was not successfully updated.\n",
      "List of analysis: [{'signature_analyzer_v1': {'pass': True, 'type': 'info', 'score': 0, 'message': 'All functions and classes in your completion are called correctly (their signatures match with the documentation).'}, 'gpt_score_analyzer_v1': {'type': 'score', 'score': 8.909126743510708, 'pass': True, 'message': ''}}, {'signature_analyzer_v1': {'pass': True, 'type': 'info', 'score': 0, 'message': 'All functions and classes in your completion are called correctly (their signatures match with the documentation).'}, 'gpt_score_analyzer_v1': {'type': 'score', 'score': 9.211558703193814, 'pass': True, 'message': ''}}]\n",
      "[LLMcoder] 0 / 0 analyzers passed\n",
      "List of analysis: [{'signature_analyzer_v1': {'pass': True, 'type': 'info', 'score': 0, 'message': 'All functions and classes in your completion are called correctly (their signatures match with the documentation).'}, 'gpt_score_analyzer_v1': {'type': 'score', 'score': 8.909126743510708, 'pass': True, 'message': ''}}, {'signature_analyzer_v1': {'pass': True, 'type': 'info', 'score': 0, 'message': 'All functions and classes in your completion are called correctly (their signatures match with the documentation).'}, 'gpt_score_analyzer_v1': {'type': 'score', 'score': 9.211558703193814, 'pass': True, 'message': ''}}]\n",
      "[LLMcoder] 0 / 0 analyzers passed\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LLMCoder._check_passing() missing 1 required positional argument: 'conversation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llmcoder\u001b[38;5;241m.\u001b[39mcomplete(conversation\u001b[38;5;241m.\u001b[39m_get_messages()[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m], temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\quinto.curso\\ai.tools\\llmcoder\\23ws-LLMcoder\\src\\llmcoder\\LLMCoder.py:178\u001b[0m, in \u001b[0;36mLLMCoder.complete\u001b[1;34m(self, code, temperature, n)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompletion generation failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# If the first completion is already correct, return it\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_passing():\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conversation\u001b[38;5;241m.\u001b[39m_get_last_message()\n\u001b[0;32m    181\u001b[0m \u001b[38;5;66;03m# Otherwise, start the feedback loop (but only if there are analyzers that can be used)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: LLMCoder._check_passing() missing 1 required positional argument: 'conversation'"
     ]
    }
   ],
   "source": [
    "llmcoder.complete(conversation._get_messages()[1][\"content\"], temperature=0.7, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " I calculated the candidates\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "candidates = llmcoder.client.chat.completions.create(messages=conversation._get_messages(), model=model, temperature=0.7, n=3)  # type: ignore\n",
    "\n",
    "print(\"\\n I calculated the candidates\")\n",
    "# Filter out completions that are repetitions of previous mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " I calculated the candidates\n",
      "[LLMcoder] Analyzing 3 completions...\n",
      "[LLMcoder] Analyzing code in separate mode...\n",
      "Generated completion 0 with score: 0\n",
      "El mensaje es: Certainly! Here's a simple code snippet in Python that prints a cute message:\n",
      "\n",
      "```python\n",
      "print(\"Hello, cutie!\")\n",
      "```\n",
      "\n",
      "Feel free to run this code to see the cute message printed on the console.\n",
      "[LLMcoder] Analyzing code in separate mode...\n",
      "[LLMcoder] Analyzing code in separate mode...\n",
      "Generated completion 1 with score: 0\n",
      "El mensaje es: Certainly! Here's a simple code snippet in Python that prints a cute message:\n",
      "\n",
      "```python\n",
      "print(\"Hello, cutie!\")\n",
      "```\n",
      "\n",
      "Feel free to run this code to see the cute message printed on the console.\n",
      "Generated completion 2 with score: 0\n",
      "El mensaje es: Certainly! Here's a simple code snippet in Python that prints a cute message:\n",
      "\n",
      "```python\n",
      "print(\"Hello, cutie!\")\n",
      "```\n",
      "\n",
      "Feel free to run this code to see the cute message printed on the console.\n",
      "Error. A conversation was not popped\n"
     ]
    }
   ],
   "source": [
    "results = llmcoder._get_completions_for(conversation, temperature = 0.7, n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
