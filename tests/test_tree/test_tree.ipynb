{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcoder.treeofcompletions.PriorityQueue import PriorityQueue, Conversation\n",
    "import openai\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \" \"\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \" Please provide a cute easy code in python \"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Please provide a cute easy code in python \n"
     ]
    }
   ],
   "source": [
    "print(messages[1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a simple code that prints a cute message:\n",
      "\n",
      "```python\n",
      "print(\"You are amazing! Keep coding and spreading cuteness!\")\n",
      "```\n",
      "\n",
      "Feel free to modify the message to make it even cuter!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from llmcoder.utils import get_openai_key\n",
    "\n",
    "client = OpenAI(api_key = get_openai_key())\n",
    "messages = client.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\", temperature=0.7, n = 3)\n",
    "print(messages.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a simple code that demonstrates a cute robotic wave using the turtle module in Python:\n",
      "\n",
      "```python\n",
      "import turtle\n",
      "\n",
      "def wave():\n",
      "    turtle.right(30)\n",
      "    for _ in range(2):\n",
      "        turtle.forward(100)\n",
      "        turtle.left(60)\n",
      "        turtle.forward(100)\n",
      "        turtle.right(120)\n",
      "\n",
      "# Setup turtle\n",
      "turtle.speed(2)\n",
      "turtle.shape(\"turtle\")\n",
      "turtle.color(\"red\")\n",
      "\n",
      "# Perform wave motion\n",
      "wave()\n",
      "\n",
      "# Clean up and terminate program\n",
      "turtle.done()\n",
      "```\n",
      "\n",
      "When you run this code, a red turtle graphic will appear on the screen and it will perform a waving motion. Feel free to make modifications and experiments to add your touch on the cute robotic wave.\n"
     ]
    }
   ],
   "source": [
    "print(messages.choices[1].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.2115587]\n"
     ]
    }
   ],
   "source": [
    "from llmcoder.analyze.GPTScoreAnalyzer import GPTScoreAnalyzer\n",
    "analyzer = GPTScoreAnalyzer()\n",
    "score1 = analyzer.score_code(messages.choices[1].message.content)\n",
    "print(score1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason='stop', index=1, logprobs=None, message=ChatCompletionMessage(content='Sure! Here\\'s a cute and easy Python code that prints a heart-shaped pattern:\\n\\n```python\\nheart = [\\n    \"  ❤️  ❤️  \",\\n    \"❤️      ❤️\",\\n    \"❤️      ❤️\",\\n    \"  ❤️  ❤️  \",\\n    \"    ❤️    \"\\n]\\n\\nfor line in heart:\\n    print(line)\\n```\\n\\nWhen you run this code, it will output a heart-shaped pattern made of heart emojis. It\\'s a simple and cute code that you can share with someone special!', role='assistant', function_call=None, tool_calls=None))\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "analyzer_results_history = \"\"\n",
    "priority_queue = PriorityQueue()\n",
    "conversation1 = Conversation(score1, messages.choices[1], analyzer_results_history)\n",
    "# conversations = [(conversation1, conversation2)]\n",
    "# priority_queue.create_priority_queue(conversations)\n",
    "priority_queue.push(conversation1)\n",
    "conversation = priority_queue.get_highest_scored_conversation()\n",
    "print(conversation._get_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": messages.choices[1].message.content,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'system',\n",
       " 'content': 'Sure! Here\\'s a cute and easy Python code that prints a heart-shaped pattern:\\n\\n```python\\nheart = [\\n    \"  ❤️  ❤️  \",\\n    \"❤️      ❤️\",\\n    \"❤️      ❤️\",\\n    \"  ❤️  ❤️  \",\\n    \"    ❤️    \"\\n]\\n\\nfor line in heart:\\n    print(line)\\n```\\n\\nWhen you run this code, it will output a heart-shaped pattern made of heart emojis. It\\'s a simple and cute code that you can share with someone special!'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Sure! Here\\'s a cute and easy Python code that prints a heart-shaped pattern:\\n\\n```python\\nheart = [\\n    \"  ❤️  ❤️  \",\\n    \"❤️      ❤️\",\\n    \"❤️      ❤️\",\\n    \"  ❤️  ❤️  \",\\n    \"    ❤️    \"\\n]\\n\\nfor line in heart:\\n    print(line)\\n```\\n\\nWhen you run this code, it will output a heart-shaped pattern made of heart emojis. It\\'s a simple and cute code that you can share with someone special!'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(message: str):\n",
    "    messages: list[dict[str, str]] = []\n",
    "    messages.append(message)\n",
    "    return messages\n",
    "\n",
    "conversation = Conversation(1, test(new_message), analyzer_results_history=[])\n",
    "conversation._get_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Here\\'s a cute and easy Python code that prints a heart-shaped pattern:\\n\\n```python\\nheart = [\\n    \"  ❤️  ❤️  \",\\n    \"❤️      ❤️\",\\n    \"❤️      ❤️\",\\n    \"  ❤️  ❤️  \",\\n    \"    ❤️    \"\\n]\\n\\nfor line in heart:\\n    print(line)\\n```\\n\\nWhen you run this code, it will output a heart-shaped pattern made of heart emojis. It\\'s a simple and cute code that you can share with someone special!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation._get_last_message()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation._add_message(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I added a new message\n"
     ]
    }
   ],
   "source": [
    "from llmcoder.LLMCoder import LLMCoder\n",
    "llmcoder = LLMCoder(log_conversation=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Certainly! Here\\'s a simple code that prints a cute message:\\n\\n```python\\nprint(\"You are amazing! Keep coding and spreading cuteness!\")\\n```\\n\\nFeel free to modify the message to make it even cuter!']\n"
     ]
    }
   ],
   "source": [
    "conversation = Conversation(score1, messages = [], analyzer_results_history=[])\n",
    "conversation._add_message(messages.choices[0].message.content)\n",
    "print(conversation._get_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I added a new message\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmcoder._add_message_to_conversation(\"user\", conversation, message = new_message, model= \"gpt-3.5-turbo\", n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': '\\'Certainly! Here\\\\\\'s a simple code snippet that prints a heart shape using ASCII art:\\\\n\\\\n```\\\\nprint(\"          ♥♥\")\\\\nprint(\"        ♥      ♥\")\\\\nprint(\"      ♥          ♥\")\\\\nprint(\"    ♥              ♥\")\\\\nprint(\"  ♥                  ♥\")\\\\nprint(\" ♥                    ♥\")\\\\nprint(\" ♥                    ♥\")\\\\nprint(\"   ♥                 ♥\")\\\\nprint(\"     ♥             ♥\")\\\\nprint(\"       ♥         ♥\")\\\\nprint(\"         ♥     ♥\")\\\\nprint(\"           ♥ ♥\")\\\\n```\\\\n\\\\nWhen you run this code, it will output a heart shape that you can enjoy!\\' is not of type \\'object\\' - \\'messages.0\\'', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m llmcoder\u001b[38;5;241m.\u001b[39m_get_completions_for(conversation, temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m      4\u001b[0m     valid_choices, analysis_results_list, candidate_scores \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32m~\\Documents\\quinto.curso\\ai.tools\\llmcoder\\23ws-LLMcoder\\src\\llmcoder\\LLMCoder.py:252\u001b[0m, in \u001b[0;36mLLMCoder._get_completions_for\u001b[1;34m(self, conversation, model, temperature, n)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03mUse OpenAI's API to get completion(s) for the user's code\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m    The completion(s) or None if all completions are repetitions of previous mistakes\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Get the completions from OpenAI's API -> 3 completions stored in candidates.choices\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mconversation\u001b[38;5;241m.\u001b[39m_get_messages(), model\u001b[38;5;241m=\u001b[39mmodel, temperature\u001b[38;5;241m=\u001b[39mtemperature, n\u001b[38;5;241m=\u001b[39mn)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m I calculated the candidates\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Filter out completions that are repetitions of previous mistakes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anaca\\miniconda3\\envs\\llmcoder_env\\Lib\\site-packages\\openai\\_utils\\_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\anaca\\miniconda3\\envs\\llmcoder_env\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:643\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    641\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    642\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    645\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    646\u001b[0m             {\n\u001b[0;32m    647\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    648\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    649\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    650\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    651\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    656\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    657\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    658\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    659\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    661\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    663\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    664\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    667\u001b[0m             },\n\u001b[0;32m    668\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    669\u001b[0m         ),\n\u001b[0;32m    670\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    671\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    672\u001b[0m         ),\n\u001b[0;32m    673\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    674\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    676\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\anaca\\miniconda3\\envs\\llmcoder_env\\Lib\\site-packages\\openai\\_base_client.py:1112\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1100\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1107\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1108\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1109\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1110\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1111\u001b[0m     )\n\u001b[1;32m-> 1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\anaca\\miniconda3\\envs\\llmcoder_env\\Lib\\site-packages\\openai\\_base_client.py:859\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    851\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    852\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    860\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    861\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    862\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    863\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    864\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    865\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\anaca\\miniconda3\\envs\\llmcoder_env\\Lib\\site-packages\\openai\\_base_client.py:949\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    946\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    948\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 949\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    952\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    953\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    957\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': '\\'Certainly! Here\\\\\\'s a simple code snippet that prints a heart shape using ASCII art:\\\\n\\\\n```\\\\nprint(\"          ♥♥\")\\\\nprint(\"        ♥      ♥\")\\\\nprint(\"      ♥          ♥\")\\\\nprint(\"    ♥              ♥\")\\\\nprint(\"  ♥                  ♥\")\\\\nprint(\" ♥                    ♥\")\\\\nprint(\" ♥                    ♥\")\\\\nprint(\"   ♥                 ♥\")\\\\nprint(\"     ♥             ♥\")\\\\nprint(\"       ♥         ♥\")\\\\nprint(\"         ♥     ♥\")\\\\nprint(\"           ♥ ♥\")\\\\n```\\\\n\\\\nWhen you run this code, it will output a heart shape that you can enjoy!\\' is not of type \\'object\\' - \\'messages.0\\'', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "results = llmcoder._get_completions_for(conversation, temperature = 0.7, n = 3)\n",
    "\n",
    "for result in results:\n",
    "    valid_choices, analysis_results_list, candidate_scores = result\n",
    "    print(candidate_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
