{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental version on llmCoder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"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\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Isolated testing of functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for exception handling\n",
    "def check_reason(reason):\n",
    "    try:\n",
    "        if reason == \"length\":\n",
    "            raise ValueError(\"Length output error.\")\n",
    "\n",
    "        if reason == \"content_filter\":\n",
    "            raise ValueError(\"Omitted content.\")\n",
    "\n",
    "        if reason == \"null\":\n",
    "            raise ValueError(\"API response still in progress.\")\n",
    "\n",
    "    except ValueError as ve:\n",
    "        # Handle the exception specific to \"reason\" scenario\n",
    "        print(f\"Error: {ve}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any other unexpected exceptions\n",
    "        print(f\"Unexpected Error: {e}\")\n",
    "    else:\n",
    "        # Execute if no exception occurred\n",
    "        print(\"No exception occurred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model_name = \"gpt-3.5-turbo\"):\n",
    "\n",
    "        # To be changed to:\n",
    "        # messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "        # {\"role\": \"user\", \"content\": user_msg}]\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = openai.ChatCompletion.create(model = model_name, messages = messages, temperature = 0)\n",
    "\n",
    "        # Check if the procedure ended correctly\n",
    "        fin_reason = response[\"choices\"][0][\"finish_reason\"]\n",
    "        # Example usage\n",
    "        try:\n",
    "            # Attempt to call the function with \"length\" as the reason\n",
    "            check_reason(fin_reason)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle any unexpected exception that might occur outside the function\n",
    "            print(f\"Unexpected Error: {e}\")\n",
    "\n",
    "        return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_first(prompt):\n",
    "        response = get_completion(prompt)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No exception occurred.\n",
      "I'm sorry, I cannot provide real-time information as I am an AI language model and do not have access to current data. However, you can check the weather in NYC today by using a weather website or app, or by searching \"NYC weather\" on a search engine.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"hello, what is the weather like in NYC today?\"\n",
    "response = complete_first(user_input)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whole-Class Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class llmCoder:\n",
    "    # @ model_name string (e.g. \"gpt-3.5-turbo\")\n",
    "    # @ feedback_invariant\n",
    "        # \"separate\" -> execution of each analyzer separately\n",
    "        # \n",
    "    def __init__(self, model_name, feedback_variant, analyzers_list):\n",
    "        self.model_name = model_name\n",
    "        self.feedback_variant = feedback_variant\n",
    "        self.analyzers_list = analyzers_list\n",
    "\n",
    "    # Function for exception handling\n",
    "    def check_reason(reason):\n",
    "        try:\n",
    "            if reason == \"length\":\n",
    "                raise ValueError(\"Length output error.\")\n",
    "\n",
    "            if reason == \"content_filter\":\n",
    "                raise ValueError(\"Omitted content.\")\n",
    "\n",
    "            if reason == \"null\":\n",
    "                raise ValueError(\"API response still in progress.\")\n",
    "\n",
    "        except ValueError as ve:\n",
    "            # Handle the exception specific to \"reason\" scenario\n",
    "            print(f\"Error: {ve}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle any other unexpected exceptions\n",
    "            print(f\"Unexpected Error: {e}\")\n",
    "        else:\n",
    "            # Execute if no exception occurred\n",
    "            print(\"No exception occurred.\")\n",
    "\n",
    "    # Function for completion \n",
    "    # a) structured prompt with system and user roles\n",
    "    # b) only system prompt\n",
    "    def get_completion(self, prompt):\n",
    "\n",
    "        # To be changed to:\n",
    "        # messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "        #                                 {\"role\": \"user\", \"content\": user_msg}]\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = openai.ChatCompletion.create(model = self.model_name, messages = messages, temperature = 0)\n",
    "\n",
    "        # Check if the procedure ended correctly\n",
    "        fin_reason = response[\"choices\"][0][\"finish_reason\"]\n",
    "        # Example usage\n",
    "        try:\n",
    "            # Attempt to call the function with \"length\" as the reason\n",
    "            check_reason(fin_reason)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle any unexpected exception that might occur outside the function\n",
    "            print(f\"Unexpected Error: {e}\")\n",
    "\n",
    "        return response.choices[0].message[\"content\"]\n",
    "\n",
    "    def complete_first(self, prompt):\n",
    "        result = self.get_completion(prompt)\n",
    "        return result\n",
    "    \n",
    "    def get_openai_api_key(self):\n",
    "        openai.api_key = os.environ(\"OPENAI_API_KEY\")\n",
    "        return openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "im in get completion\n",
      "\n",
      "hello!!!!\n",
      "No exception occurred.\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Parse the command line arguments for commands and options\n",
    "\n",
    "    Commands:\n",
    "    `llmcoder fine-tune-autocomplete` - Scrape & preprocess data for fine-tuning the autocomplete model\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the command line arguments for commands and options\n",
    "    parser = argparse.ArgumentParser(description = 'LLMcoder - Feedback-Based Coding Assistant')\n",
    "    parser.add_argument('command', help = 'Command to execute')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Execute the command\n",
    "    # match args.command:\n",
    "    if(args.command =='fine-tune-autocomplete'):\n",
    "        # from llmcoder.fine_tune import fine_tune_autocomplete\n",
    "        # fine_tune_autocomplete()  # FIXME: Implement fine-tune-autocomplete\n",
    "        # Creating an instance of LLMCoder\n",
    "        llm_coder_instance = llmCoder(model_name = \"gpt-3.5-turbo\", feedback_variant = \"separate\", analyzers_list = {})\n",
    "        user_input = \"hello\"\n",
    "        result = llm_coder_instance.complete_first(user_input)\n",
    "        print(result)\n",
    "\n",
    "    else:\n",
    "        print('Unknown command: ', args.command)\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(code_suggestion: str) -> str:\n",
    "             # ...\n",
    "            return f\"Syntax check applied to: \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Syntax check applied to: '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze(code_suggestion = \"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_suggestion = \"hello\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
