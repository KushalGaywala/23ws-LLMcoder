{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcoder import LLMCoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 0: Add a middleware to FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_code_0 = '''import argparse\n",
    "\n",
    "import uvicorn\n",
    "from fastapi import APIRouter, FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "\n",
    "def api(args: argparse.Namespace) -> FastAPI:\n",
    "    app = FastAPI(\n",
    "        title=\"My Title\",\n",
    "        description=\"My Description\",\n",
    "        version=\"0.1.0\",\n",
    "    )\n",
    "\n",
    "    app.add_middleware(\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmcoder = LLMCoder(\n",
    "    analyzers=[],\n",
    "    max_iter=1,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMcoder] Creating first completions...\n",
      "[LLMcoder] Choosing conversation R with score 0\n",
      "[LLMcoder] Analyzing completion...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Have 1 conversations:\n",
      "[LLMcoder] Passing   Score     Prob      Path\n",
      "[LLMcoder] True      0         1.0       ['R', 0]\n",
      "[LLMcoder] First completion is correct. Stopping early...\n"
     ]
    }
   ],
   "source": [
    "# By default, only generate one completion\n",
    "completion = llmcoder.complete(code=example_code_0, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORSMiddleware,\n",
      "        allow_origins=[\"*\"],\n",
      "        allow_credentials=True,\n",
      "        allow_methods=[\"*\"],\n",
      "        allow_headers=[\"*\"],\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "# Final Completion\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- [SYSTEM] --------------------\n",
      "You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\n",
      "You are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\n",
      "Precisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\n",
      "Your code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\n",
      "Your unique capability is to provide completions without altering, repeating, or commenting on the original code.\n",
      "You offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\n",
      "This approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\n",
      "Your response begins with the next characters of the line if the last line of the user's code is incomplete, or the next line if the last line of the user's code is complete.\n",
      "Your application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\n",
      "-------------------- [USER] --------------------\n",
      "import argparse\n",
      "\n",
      "import uvicorn\n",
      "from fastapi import APIRouter, FastAPI\n",
      "from fastapi.middleware.cors import CORSMiddleware\n",
      "\n",
      "\n",
      "def api(args: argparse.Namespace) -> FastAPI:\n",
      "    app = FastAPI(\n",
      "        title=\"My Title\",\n",
      "        description=\"My Description\",\n",
      "        version=\"0.1.0\",\n",
      "    )\n",
      "\n",
      "    app.add_middleware(\n",
      "        \n",
      "-------------------- [ASSISTANT] --------------------\n",
      "CORSMiddleware,\n",
      "        allow_origins=[\"*\"],\n",
      "        allow_credentials=True,\n",
      "        allow_methods=[\"*\"],\n",
      "        allow_headers=[\"*\"],\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "# Conversation\n",
    "for message in llmcoder.conversations[0].messages:\n",
    "    print(f'-------------------- [{message[\"role\"].upper()}] --------------------')\n",
    "    print(message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Create an LLMCoder object with the right parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_code_1 = '''from llmcoder import LLMCoder\n",
    "\n",
    "# Create an LLMCoder with backtracking\n",
    "llmcoder = LLMCoder('''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmcoder = LLMCoder(\n",
    "    analyzers=[\"mypy_analyzer_v1\", \"signature_analyzer_v1\"],\n",
    "    max_iter=1,\n",
    "    feedback_variant=\"coworker\",  # Pass messages from the MyPy analyzer to the Signature analyzer\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMcoder] Creating first completions...\n",
      "[LLMcoder] Choosing conversation R with score 0\n",
      "[LLMcoder] Analyzing completion...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running mypy_analyzer_v1...\n",
      "[Mypy] No missing stubs found.\n",
      "[Mypy] /tmp/tmpgpaogg8u.py:4: error: Unexpected keyword argument \"backtrack\" for \"LLMCoder\"; did you mean \"backtracking\"?  [call-arg]\n",
      "[Mypy] /home/psaegert/Projects/23ws-LLMcoder/src/llmcoder/llmcoder.py:41: note: \"LLMCoder\" defined here\n",
      "[Mypy] Found 1 error in 1 file (checked 1 source file)\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] Using context from previous analyzers: ['mypy_analyzer_v1']\n",
      "[Signatures] Found problematic function or class: LLMCoder\n",
      "[Signatures] function_calls=[(None, 'LLMCoder', None)]\n",
      "[Signatures] Got signatures and documentations:\n",
      "[Signatures] LLMCoder: (analyzers: list[str] = None, model_first: str = 'ft:gpt-3.5-turbo-1106:personal::8LCi9Q0d', model_feedback: str = 'ft:gpt-3.5-turbo-1106:personal::8LCi9Q0d', feedback_variant: str = 'coworker', system_prompt: str | None = None, max_iter: int = 10, backtracking: bool = True, log_conversation: bool = False, n_procs: int = 1, verbose: bool = True) -> None, Initialize the LLMCoder\n",
      "[LLMcoder] Have 1 conversations:\n",
      "[LLMcoder] Passing   Score     Prob      Path\n",
      "[LLMcoder] False     -2        1.0       ['R', 0]\n",
      "[LLMcoder] Starting feedback loop...\n",
      "[LLMcoder] Starting feedback iteration 1...\n",
      "[LLMcoder] Choosing conversation R-0 with score -2\n",
      "[LLMcoder] Analyzing completion...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running mypy_analyzer_v1...\n",
      "[Mypy] No missing stubs found.\n",
      "[Mypy] Success: no issues found in 1 source file\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] Using context from previous analyzers: ['mypy_analyzer_v1']\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[LLMcoder] Have 1 conversations:\n",
      "[LLMcoder] Passing   Score     Prob      Path\n",
      "[LLMcoder] True      0         1.0       ['R', 0, 0]\n",
      "[LLMcoder] Code is correct. Stopping early...\n"
     ]
    }
   ],
   "source": [
    "# Analyze four completions in parallel\n",
    "completion = llmcoder.complete(example_code_1, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backtracking=True)\n"
     ]
    }
   ],
   "source": [
    "# Final Completion\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- [SYSTEM] --------------------\n",
      "You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\n",
      "You are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\n",
      "Precisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\n",
      "Your code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\n",
      "Your unique capability is to provide completions without altering, repeating, or commenting on the original code.\n",
      "You offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\n",
      "This approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\n",
      "Your response begins with the next characters of the line if the last line of the user's code is incomplete, or the next line if the last line of the user's code is complete.\n",
      "Your application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\n",
      "-------------------- [USER] --------------------\n",
      "from llmcoder import LLMCoder\n",
      "\n",
      "# Create an LLMCoder with backtracking\n",
      "llmcoder = LLMCoder(\n",
      "-------------------- [ASSISTANT] --------------------\n",
      "backtrack=True)\n",
      "-------------------- [USER] --------------------\n",
      "[INST]\n",
      "The completion you provided resulted in the following errors:\n",
      "your completion:4: error: Unexpected keyword argument \"backtrack\" for \"LLMCoder\"; did you mean \"backtracking\"?  [call-arg]\n",
      "/home/psaegert/Projects/23ws-LLMcoder/src/llmcoder/llmcoder.py:41: note: \"LLMCoder\" defined here\n",
      "Found 1 error in 1 file (checked 1 source file)\n",
      "To fix these errors, use these ground truth signatures as a reference for your next completion:\n",
      "LLMCoder: (analyzers: list[str] = None, model_first: str = 'ft:gpt-3.5-turbo-1106:personal::8LCi9Q0d', model_feedback: str = 'ft:gpt-3.5-turbo-1106:personal::8LCi9Q0d', feedback_variant: str = 'coworker', system_prompt: str | None = None, max_iter: int = 10, backtracking: bool = True, log_conversation: bool = False, n_procs: int = 1, verbose: bool = True) -> None\n",
      "\n",
      "Fix, improve and rewrite your completion for the following code:\n",
      "[/INST]\n",
      "from llmcoder import LLMCoder\n",
      "\n",
      "# Create an LLMCoder with backtracking\n",
      "llmcoder = LLMCoder(\n",
      "-------------------- [ASSISTANT] --------------------\n",
      "backtracking=True)\n"
     ]
    }
   ],
   "source": [
    "# Conversation\n",
    "for message in llmcoder.conversations[0].messages:\n",
    "    print(f'-------------------- [{message[\"role\"].upper()}] --------------------')\n",
    "    print(message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Create a TikToken encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_code_2 = '''import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmcoder = LLMCoder(\n",
    "    analyzers=[\"mypy_analyzer_v1\", \"signature_analyzer_v1\", \"hallucination_analyzer_v1\", \"gpt_score_analyzer_v1\"],\n",
    "    max_iter=5,\n",
    "    feedback_variant=\"coworker\",  # Pass messages from the MyPy analyzer to the Signature analyzer and Hallucination analyzer\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMcoder] Creating first completions...\n",
      "[LLMcoder] Choosing conversation R with score 0\n",
      "[LLMcoder] Analyzing 3 completions...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running mypy_analyzer_v1...\n",
      "[Mypy] No missing stubs found.\n",
      "[Mypy] /tmp/tmp90pys0vu.py:7: error: Module has no attribute \"TikTokToken\"  [attr-defined]\n",
      "[Mypy] Found 1 error in 1 file (checked 1 source file)\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] Using context from previous analyzers: ['mypy_analyzer_v1']\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[LLMcoder] Running hallucination_analyzer_v1...\n",
      "[Hallucinations] Using context from previous analyzers: ['mypy_analyzer_v1', 'signature_analyzer_v1']\n",
      "[Hallucinations] Found problematic function or class: TikTokToken\n",
      "[Hallucinations] Found 1 hallucinations: ['tiktoken.TikTokToken']\n",
      "[LLMcoder] Running gpt_score_analyzer_v1...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running mypy_analyzer_v1...\n",
      "[Mypy] No missing stubs found.\n",
      "[Mypy] /tmp/tmp0un3ivhs.py:7: error: Module has no attribute \"TikTok\"  [attr-defined]\n",
      "[Mypy] Found 1 error in 1 file (checked 1 source file)\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] Using context from previous analyzers: ['mypy_analyzer_v1']\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[LLMcoder] Running hallucination_analyzer_v1...\n",
      "[Hallucinations] Using context from previous analyzers: ['mypy_analyzer_v1', 'signature_analyzer_v1']\n",
      "[Hallucinations] Found problematic function or class: TikTok\n",
      "[Hallucinations] Found 1 hallucinations: ['tiktoken.TikTok']\n",
      "[LLMcoder] Running gpt_score_analyzer_v1...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running mypy_analyzer_v1...\n",
      "[Mypy] No missing stubs found.\n",
      "[Mypy] /tmp/tmp82bodgbn.py:7: error: Module has no attribute \"TikIDToken\"  [attr-defined]\n",
      "[Mypy] /tmp/tmp82bodgbn.py:7: error: Name \"token\" is not defined  [name-defined]\n",
      "[Mypy] Found 2 errors in 1 file (checked 1 source file)\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] Using context from previous analyzers: ['mypy_analyzer_v1']\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[LLMcoder] Running hallucination_analyzer_v1...\n",
      "[Hallucinations] Using context from previous analyzers: ['mypy_analyzer_v1', 'signature_analyzer_v1']\n",
      "[Hallucinations] Found problematic function or class: TikIDToken\n",
      "[Hallucinations] Found 1 hallucinations: ['tiktoken.TikIDToken']\n",
      "[LLMcoder] Running gpt_score_analyzer_v1...\n",
      "[LLMcoder] Have 3 conversations:\n",
      "[LLMcoder] Passing   Score     Prob      Path\n",
      "[LLMcoder] False     7.71      1.0       ['R', 0]\n",
      "[LLMcoder] False     5.13      0.0       ['R', 1]\n",
      "[LLMcoder] False     2.31      0.0       ['R', 2]\n",
      "[LLMcoder] Starting feedback loop...\n",
      "[LLMcoder] Starting feedback iteration 1...\n",
      "[LLMcoder] Choosing conversation R-0 with score 7.71\n",
      "[LLMcoder] Analyzing 3 completions...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running mypy_analyzer_v1...\n",
      "[Mypy] No missing stubs found.\n",
      "[Mypy] /tmp/tmprk68dzlr.py:7: error: Module has no attribute \"tiktoken\"  [attr-defined]\n",
      "[Mypy] Found 1 error in 1 file (checked 1 source file)\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] Using context from previous analyzers: ['mypy_analyzer_v1']\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[LLMcoder] Running hallucination_analyzer_v1...\n",
      "[Hallucinations] Using context from previous analyzers: ['mypy_analyzer_v1', 'signature_analyzer_v1']\n",
      "[Hallucinations] Found problematic function or class: tiktoken\n",
      "[Hallucinations] Found 1 hallucinations: ['tiktoken.tiktoken']\n",
      "[LLMcoder] Running gpt_score_analyzer_v1...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running mypy_analyzer_v1...\n",
      "[Mypy] No missing stubs found.\n",
      "[Mypy] /tmp/tmphaa2msz1.py:7: error: Missing positional argument \"name\" in call to \"Encoding\"  [call-arg]\n",
      "[Mypy] /tmp/tmphaa2msz1.py:7: error: Missing named argument \"pat_str\" for \"Encoding\"  [call-arg]\n",
      "[Mypy] /tmp/tmphaa2msz1.py:7: error: Missing named argument \"mergeable_ranks\" for \"Encoding\"  [call-arg]\n",
      "[Mypy] /tmp/tmphaa2msz1.py:7: error: Missing named argument \"special_tokens\" for \"Encoding\"  [call-arg]\n",
      "[Mypy] Found 4 errors in 1 file (checked 1 source file)\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] Using context from previous analyzers: ['mypy_analyzer_v1']\n",
      "[Signatures] Found problematic function or class: Encoding\n",
      "[Signatures] Found problematic function or class: Encoding\n",
      "[Signatures] Found problematic function or class: Encoding\n",
      "[Signatures] function_calls=[(None, 'Encoding', None)]\n",
      "[Signatures] No import found for Encoding\n",
      "[Signatures] Got signatures and documentations:\n",
      "[LLMcoder] Running hallucination_analyzer_v1...\n",
      "[Hallucinations] Using context from previous analyzers: ['mypy_analyzer_v1', 'signature_analyzer_v1']\n",
      "[Hallucinations] No hallucinations found.\n",
      "[LLMcoder] Running gpt_score_analyzer_v1...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running mypy_analyzer_v1...\n",
      "[Mypy] No missing stubs found.\n",
      "[Mypy] Success: no issues found in 1 source file\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] Using context from previous analyzers: ['mypy_analyzer_v1']\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[LLMcoder] Running hallucination_analyzer_v1...\n",
      "[Hallucinations] Using context from previous analyzers: ['mypy_analyzer_v1', 'signature_analyzer_v1']\n",
      "[Hallucinations] No hallucinations found.\n",
      "[LLMcoder] Running gpt_score_analyzer_v1...\n",
      "[LLMcoder] Have 5 conversations:\n",
      "[LLMcoder] Passing   Score     Prob      Path\n",
      "[LLMcoder] True      5.38      1.0       ['R', 0, 2]\n",
      "[LLMcoder] False     5.13      0.0       ['R', 1]\n",
      "[LLMcoder] False     4.88      0.0       ['R', 0, 0]\n",
      "[LLMcoder] False     2.31      0.0       ['R', 2]\n",
      "[LLMcoder] False     1.38      0.0       ['R', 0, 1]\n",
      "[LLMcoder] Code is correct. Stopping early...\n"
     ]
    }
   ],
   "source": [
    "# Analyze four completions in parallel\n",
    "completion = llmcoder.complete(example_code_2, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_encoding(\"hello world\")\n"
     ]
    }
   ],
   "source": [
    "# Final Completion\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- [SYSTEM] --------------------\n",
      "You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\n",
      "You are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\n",
      "Precisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\n",
      "Your code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\n",
      "Your unique capability is to provide completions without altering, repeating, or commenting on the original code.\n",
      "You offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\n",
      "This approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\n",
      "Your response begins with the next characters of the line if the last line of the user's code is incomplete, or the next line if the last line of the user's code is complete.\n",
      "Your application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\n",
      "-------------------- [USER] --------------------\n",
      "import json\n",
      "import os\n",
      "\n",
      "import numpy as np\n",
      "import tiktoken\n",
      "\n",
      "enc = tiktoken.\n",
      "-------------------- [ASSISTANT] --------------------\n",
      "TikTokToken()\n",
      "-------------------- [USER] --------------------\n",
      "[INST]\n",
      "The completion you provided resulted in the following errors:\n",
      "your completion:7: error: Module has no attribute \"TikTokToken\"  [attr-defined]\n",
      "Found 1 error in 1 file (checked 1 source file)\n",
      "\n",
      "The following attributes do not exist:\n",
      "- 'TikTokToken' of 'tiktoken'\n",
      "\n",
      "Do not use these attributes.\n",
      "\n",
      "Instead of 'tiktoken.TikTokToken', use the most plausible of these attributes: tiktoken.[Encoding, encoding_for_model, encoding_name_for_model, get_encoding, list_encoding_names, tiktoken._tiktoken, _educational, model, registry, core, load, __package__, __doc__, __name__, __file__]\n",
      "\n",
      "\n",
      "Fix, improve and rewrite your completion for the following code:\n",
      "[/INST]\n",
      "import json\n",
      "import os\n",
      "\n",
      "import numpy as np\n",
      "import tiktoken\n",
      "\n",
      "enc = tiktoken.\n",
      "-------------------- [ASSISTANT] --------------------\n",
      "get_encoding(\"hello world\")\n"
     ]
    }
   ],
   "source": [
    "# Conversation\n",
    "for message in llmcoder.conversations[0].messages:\n",
    "    print(f'-------------------- [{message[\"role\"].upper()}] --------------------')\n",
    "    print(message['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
