{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "from llmcoder.utils import get_openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=get_openai_key())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"quality_scores = []\n",
    "needs_comment_scores = []\n",
    "\n",
    "for answer in tqdm(answer_list):\n",
    "    full_code = code + answer\n",
    "\n",
    "    # If the full_code is longer than 512 tokens, we need to truncate it\n",
    "    while len(tokenizer.encode(full_code, return_tensors=\"pt\")[0]) > 512:\n",
    "        full_code = full_code[1:]\n",
    "\n",
    "    input_ids = tokenizer.encode(full_code, return_tensors=\"pt\").to(device)\n",
    "    output = model(input_ids)\n",
    "    quality_scores.append(output.logits[0, 0].to(\"cpu\").tolist())\n",
    "    needs_comment_scores.append(output.logits[0, 1].to(\"cpu\").tolist())\"\"\"\n",
    "\n",
    "code11 = \"\"\"# Set up empty arrays\n",
    "quality_scores = []\n",
    "needs_comment_scores = []\n",
    "\n",
    "# Loop through each answer\n",
    "for answer in tqdm(answer_list):\n",
    "    full_code = code + answer\n",
    "\n",
    "    # If the full_code is longer than 512 tokens, we need to truncate it\n",
    "    while len(tokenizer.encode(full_code, return_tensors=\"pt\")[0]) > 512:\n",
    "        full_code = full_code[1:]\n",
    "\n",
    "    input_ids = tokenizer.encode(full_code, return_tensors=\"pt\").to(device)\n",
    "    output = model(input_ids)\n",
    "    quality_scores.append(output.logits[0, 0].to(\"cpu\").tolist())\n",
    "    needs_comment_scores.append(output.logits[0, 1].to(\"cpu\").tolist())\"\"\"\n",
    "\n",
    "code2 = \"\"\"quality_scores = []\n",
    "needs_comment_scores = []\n",
    "\n",
    "for answer in tqdm(answer_list):\n",
    "    full_code = code + answer\n",
    "\n",
    "    # If the full_code is longer than 512 tokens, we need to truncate it\n",
    "    while len(tokenizer.encode(full_code, return_tensors=\"pt\")[0]) > 512:\n",
    "        full_code = full_code[1:]\n",
    "\n",
    "    input_ids = tokenizer.encode(full_code, return_tensors=\"pt\").to(device)\n",
    "    output = model(input_ids)\n",
    "    quality_scores.append(output.logits[0, 0].to(\"cpu\").tolist())\n",
    "    needs_comment_scores.append(output.logits[0, 1].to(\"cpu\").tolist())\n",
    "\n",
    "input_ids = tokenizer.encode(code, return_tensors=\"pt\").to(device)\n",
    "quality_score_before = model(input_ids).logits[0, 0].cpu().tolist()\n",
    "needs_comment_score_before = model(input_ids).logits[0, 1].cpu().tolist()\"\"\"\n",
    "\n",
    "code3 = \"\"\"quality_scores = []\n",
    "needs_comment_scores = []\n",
    "\n",
    "for answer in tqdm(answer_list):\n",
    "    full_code = code + answer\n",
    "\n",
    "    # If the full_code is longer than 512 tokens, we need to truncate it\n",
    "    while len(tokenizer.encode(full_code, return_tensors=\"pt\")[0]) > 512:\n",
    "        full_code = full_code[1:]\n",
    "\n",
    "    input_ids = tokenizer.encode(full_code, return_tensors=\"pt\").to(device)\n",
    "    output = model(input_ids)\n",
    "    quality_scores.append(output.logits[0, 0].to(\"cpu\").tolist())\n",
    "    needs_comment_scores.append(output.logits[0, 1].to(\"cpu\").tolist())\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "code4 = \"\"\"quality_scores = []\n",
    "needs_comment_scores = []\n",
    "\n",
    "for answer in tqdm(answer_list):\n",
    "    full_code = code + answer\n",
    "\n",
    "    If the full_code is longer than 512 tokens, we need to truncate it\n",
    "    while len(tokenizer.encode(full_code, return_tensors=\"pt\")[0]) > 512:\n",
    "        full_code = full_code[1:]\n",
    "\n",
    "    input_ids = tokenizer.encode(full_code, return_tensors=\"pt\").to(device)\n",
    "    output = model(input_ids)\n",
    "    quality_scores.append(output.logits[0, 0].to(\"cpu\").tolist())\n",
    "    needs_comment_scores.append(output.logits[0, 1].to(\"cpu\").tolist())\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "codes = [code, code11, code2, code3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_prompt(code_list: list[str]) -> str:\n",
    "    \"\"\"Concatenates the code snippets with the scoring prompt in the following format:\n",
    "\n",
    "    Code snippet 1:\n",
    "    ```python\n",
    "    <code>\n",
    "    ```\n",
    "\n",
    "    Code snippet 2:\n",
    "    ```python\n",
    "    <code>\n",
    "    ```\n",
    "    ...\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\n",
    "    for i, code in enumerate(code_list):\n",
    "        prompt += f\"Code snippet {i + 1}:\\n```python\\n{code}\\n```\\n\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "@staticmethod\n",
    "def score_code(code: str | list[str], client: openai.OpenAI, scoring_prompt: str, reduction: str = \"geo\") -> float:\n",
    "    if isinstance(code, str):\n",
    "        code = [code]\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": scoring_prompt\n",
    "        }, {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": score_prompt(code)\n",
    "        }\n",
    "    ]\n",
    "    completions = client.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "    lines = completions.choices[0].message.content.split(\"\\n\")\n",
    "\n",
    "    print(completions.choices[0].message.content)\n",
    "\n",
    "    # Extract the scores from the response\n",
    "    scores = []\n",
    "    if \"Code snippet\" in lines[0]:\n",
    "        for i, line in enumerate(lines):\n",
    "            scores_for_snippet = []\n",
    "            if line.startswith(\"Code snippet\"):\n",
    "                for j in range(3):\n",
    "                    try:\n",
    "                        scores_for_snippet.append(float(lines[i + j + 1][lines[i + j + 1].index(\":\") + 1:]))\n",
    "                    except ValueError:\n",
    "                        print(f\"[Scoring] Error while scoring code. Expected float, got: {completions.choices[0].message.content}\")\n",
    "                        scores_for_snippet.append(np.nan)\n",
    "                scores.append(scores_for_snippet)\n",
    "    elif len(code) == 1:\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                scores.append(float(lines[i][lines[i].index(\":\") + 1:]))\n",
    "            except ValueError:\n",
    "                print(f\"[Scoring] Error while scoring code. Expected float, got: {completions.choices[0].message.content}\")\n",
    "                scores.append(np.nan)\n",
    "\n",
    "    scores = np.atleast_2d(np.array(scores))\n",
    "\n",
    "    match reduction:\n",
    "        case \"mean\":\n",
    "            return scores.mean(axis=1)\n",
    "        case \"max\":\n",
    "            return scores.max(axis=1)\n",
    "        case \"geo\":\n",
    "            return scores.prod(axis=1) ** (1 / scores.shape[1])\n",
    "        case _:\n",
    "            raise ValueError(\"Invalid reduction method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_prompt = \"\"\"You are a scoring system for python code that meticulously analyzes a code snippet and judges its quality in given categories.\n",
    "You are given a python code snippet and asked to score it based on the following categories:\n",
    "- the code quality: How well the code conforms to the python style guide\n",
    "- the plausibliity of the last few lines: How much sense the last few lines make\n",
    "- the consistency of the last few lines: How good the last few lines fit to the beginning and middle of the code\n",
    "- the readability of the code: How easy it is to understand the code\n",
    "\n",
    "You are very critical and unforgiving, and you will give a low score if the code is not perfect. You will give a high score if the code is perfect.\n",
    "\n",
    "Respond in the following pattern:\n",
    "\n",
    "```\n",
    "Code Quality: <code quality score>\n",
    "Last Lines Plausibility: <plausibility score>\n",
    "Last Liens Consistency: <consistency score>\n",
    "Code Readability: <readability score>\n",
    "```\n",
    "\n",
    "If you are given multiple code snippets of the format, respond in the following pattern:\n",
    "\n",
    "```\n",
    "Code snippet 1:\n",
    "Code Quality: <code quality score>\n",
    "Last Lines Plausibility: <plausibility score>\n",
    "Last Liens Consistency: <consistency score>\n",
    "Code Readability: <readability score>\n",
    "\n",
    "Code snippet 2:\n",
    "Code Quality: <code quality score>\n",
    "Last Lines Plausibility: <plausibility score>\n",
    "Last Liens Consistency: <consistency score>\n",
    "Code Readability: <readability score>\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "If there are any differences in the code snippets, reflect them in your scores: Give higher scores to code snippets that handle a category better than others and lower scores to code snippets that handle a category worse than others.\n",
    "\n",
    "Your answer must always have this pattern. Do not deviate from this pattern and respond with a score between 0 and 10 for each category and no other information.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Quality: 7.5\n",
      "Last Lines Plausibility: 8.0\n",
      "Last Lines Consistency: 8.0\n",
      "Code Readability: 8.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7.82973528])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_code(codes[0], client, scoring_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
