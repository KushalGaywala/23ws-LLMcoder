{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcoder import LLMCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmcoder = LLMCoder(analyzers=[\"mypy_analyzer_v1\", \"signature_analyzer_v1\"], max_iter=3, feedback_variant=\"coworker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = ['''from llmcoder import LLMCoder\n",
    "\n",
    "# Create an LLMCoder with a random seed of 0\n",
    "llmcoder = ''',\n",
    "'''import os\n",
    "from aleph_alpha_client import Client, CompletionRequest, Prompt\n",
    "\n",
    "\n",
    "\n",
    "client = Client(os.''', # https://github.com/Aleph-Alpha/aleph-alpha-client\n",
    "'''# Import random forest regressor\n",
    "''',\n",
    "'''import os\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "from llmcoder.analyze.Analyzer import Analyzer\n",
    "\n",
    "\n",
    "class MypyAnalyzer(Analyzer):\n",
    "    def analyze(self, input: str, completion: str, install_stubs: bool = True, *mypy_args: str | None) -> dict:\n",
    "        code = input + completion\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".py\", mode=\"w\") as temp_file:\n",
    "            temp_file_name = temp_file.name\n",
    "            temp_file.write(code)\n",
    "\n",
    "        if mypy_args is None:\n",
    "            # mypy_args = [\"--disable-error-code=import-untyped\"]  #FiXME: Should resolve error: Skipping analyzing \"sklearn.ensemble\": module is installed, but missing library stubs or py.typed marker  [import-untyped] but doesn't\n",
    "            mypy_args = []\n",
    "\n",
    "        # First run of mypy to check for missing stubs\n",
    "        mypy_run = subprocess.run([\"mypy\", temp_file_name, *mypy_args], capture_output=True, text=True)\n",
    "\n",
    "        # Check if mypy reported missing stubs\n",
    "        indicators = [\"install-types\"]\n",
    "        missing_stubs = any(indicator in mypy_run.stdout or indicator in mypy_run.stderr for indicator in indicators)\n",
    "\n",
    "        # Install stubs if missing\n",
    "        if install_stubs and missing_stubs:\n",
    "            print(\"Installing missing stubs...\")\n",
    "\n",
    "            # Install missing stubs\n",
    "            subprocess.run([\"mypy\", \"--install-types\", \"--non-interactive\", *mypy_args], capture_output=True, text=True)\n",
    "\n",
    "            # Re-run mypy after installing stubs\n",
    "            mypy_run = subprocess.run([\"mypy\", temp_file_name, *mypy_args], capture_output=True, text=True)\n",
    "        else:\n",
    "            print(\"No missing stubs found.\")\n",
    "\n",
    "        result = mypy_run.stdout if mypy_run.stdout else mypy_run.stderr\n",
    "\n",
    "        # Get the number of lines of the input code\n",
    "        n_input_lines = len(input.split(\"\\n\")) - 1\n",
    "\n",
    "        # Parse the line number from the mypy error message and filter the result\n",
    "        filtered_result = []\n",
    "        for line in result.split(\"\\n\"):\n",
    "            if line.startswith(temp_file_name):\n",
    "                line_number = int(line.split(\":\")[1])\n",
    "                if line_number > n_input_lines:\n",
    "                    filtered_result.append(line)\n",
    "            else:\n",
    "                filtered_result.append(line)\n",
    "\n",
    "        # Replace the temp file name with \"your completion\". This helps the LLM understand that the error is caused by its completion.\n",
    "        filtered_result = [line.replace(temp_file_name, \"your completion\") for line in filtered_result]\n",
    "\n",
    "        # Remove the error message \"your completion:2: \\x1b[1m\\x1b[31merror:\\x1b[m Skipping analyzing...\" since it cannot be resolved by installing stubs\n",
    "        filter''',\n",
    "'''import argparse\n",
    "\n",
    "import uvicorn\n",
    "from fastapi import APIRouter, FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "from .routers import models, predict\n",
    "\n",
    "\n",
    "def api(args: argparse.Namespace) -> FastAPI:\n",
    "    app = FastAPI(\n",
    "        title=\"PubMed Research Trends\",\n",
    "        description=\"PubMed Research Trends is a web application that allows users to explore trends in biomedical research. Users can search for topics and visualize the trends in the number of publications over time. The application also provides a topic clustering feature that allows users to explore the topics that are related to a given topic.\",\n",
    "        version=\"0.1.0\",\n",
    "    )\n",
    "\n",
    "    app.add_middleware(''',\n",
    "'''from langchain.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embed and store\n",
    "from langchain.vectorstores.elasticsearch import ElasticsearchStore \n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings # We can also try Ollama embeddings\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# pip install jq\n",
    "schema = {\n",
    "    'jq_schema': '.text'\n",
    "}\n",
    "\n",
    "es = get_es_connection()\n",
    "\n",
    "REINDEX = False\n",
    "\n",
    "# pip install gpt4all\n",
    "# Needs Ubuntu > 22.04 because of glibc\n",
    "\n",
    "if REINDEX:\n",
    "    # Clear the index\n",
    "    es.indices.delete(index=\"eurlex-langchain\", ignore=[400, 404])\n",
    "\n",
    "    start_time = time.time()\n",
    "    vectorstore = '''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating first completion...\n",
      "Starting feedback loop...\n",
      "Starting feedback iteration 1...\n",
      "Analyzing code in a coworker mode...\n",
      "Running mypy_analyzer_v1...\n",
      "No missing stubs found.\n",
      "/tmp/tmpybr6z05x.py:27: error: Name \"get_es_connection\" is not defined  [name-defined]\n",
      "/tmp/tmpybr6z05x.py:39: error: Too many positional arguments for \"ElasticsearchStore\"  [misc]\n",
      "/tmp/tmpybr6z05x.py:39: error: Argument 2 to \"ElasticsearchStore\" has incompatible type \"str\"; expected \"Embeddings | None\"  [arg-type]\n",
      "Found 3 errors in 1 file (checked 1 source file)\n",
      "\n",
      "Running signature_analyzer_v1...\n",
      "Using context from previous analyzers: ['mypy_analyzer_v1']\n",
      "Found problematic function or class: ElasticsearchStore\n",
      "Found problematic function or class: ElasticsearchStore\n",
      "Name ElasticsearchStore matches query ['ElasticsearchStore']\n",
      "Got signatures:\n",
      "(index_name: str, *, embedding: Optional[langchain_core.embeddings.Embeddings] = None, es_connection: Optional[ForwardRef('Elasticsearch')] = None, es_url: Optional[str] = None, es_cloud_id: Optional[str] = None, es_user: Optional[str] = None, es_api_key: Optional[str] = None, es_password: Optional[str] = None, vector_query_field: str = 'vector', query_field: str = 'text', distance_strategy: Optional[Literal[<DistanceStrategy.COSINE: 'COSINE'>, <DistanceStrategy.DOT_PRODUCT: 'DOT_PRODUCT'>, <DistanceStrategy.EUCLIDEAN_DISTANCE: 'EUCLIDEAN_DISTANCE'>]] = None, strategy: langchain.vectorstores.elasticsearch.BaseRetrievalStrategy = <langchain.vectorstores.elasticsearch.ApproxRetrievalStrategy object at 0x7f45c4e0c8d0>)\n",
      "0 / 1 analyzers passed\n",
      "Starting feedback iteration 2...\n",
      "Analyzing code in a coworker mode...\n",
      "Running mypy_analyzer_v1...\n",
      "No missing stubs found.\n",
      "/tmp/tmpmuli93z0.py:27: error: Name \"get_es_connection\" is not defined  [name-defined]\n",
      "Found 1 error in 1 file (checked 1 source file)\n",
      "\n",
      "Running signature_analyzer_v1...\n",
      "Using context from previous analyzers: ['mypy_analyzer_v1']\n",
      "No problematic functions or classes found in the context.\n",
      "1 / 1 analyzers passed\n"
     ]
    }
   ],
   "source": [
    "result = llmcoder.complete(code[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ElasticsearchStore(\"eurlex-langchain\", embedding=OllamaEmbeddings(), es_connection=es)\\n    llm = Ollama(OllamaEmbeddings(), vectorstore)\\n    llm.load(\"eurlex-ollama.v1\", \"/mnt/data/eurlex-ollama.v1\")\\n    print(\"Time to load LLM:\", time.time()-start_time)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM\n",
      "You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\n",
      "You are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\n",
      "Precisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\n",
      "Your code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\n",
      "Your unique capability is to provide completions without altering, repeating, or commenting on the original code.\n",
      "You offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\n",
      "This approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\n",
      "Your response begins with the next characters of the line if the last line of the user'S code is incomplete, or the next line if the last line of the user's code is complete.\n",
      "Your application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\n",
      "--------------------------------------------------------------------------------\n",
      "USER\n",
      "from langchain.document_loaders import DirectoryLoader, JSONLoader\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "\n",
      "# Embed and store\n",
      "from langchain.vectorstores.elasticsearch import ElasticsearchStore \n",
      "from langchain.embeddings import GPT4AllEmbeddings\n",
      "from langchain.embeddings import OllamaEmbeddings # We can also try Ollama embeddings\n",
      "\n",
      "from langchain.llms import Ollama\n",
      "from langchain.callbacks.manager import CallbackManager\n",
      "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
      "\n",
      "import os\n",
      "import json\n",
      "from tqdm import tqdm\n",
      "from bs4 import BeautifulSoup\n",
      "import time\n",
      "\n",
      "import torch\n",
      "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "# pip install jq\n",
      "schema = {\n",
      "    'jq_schema': '.text'\n",
      "}\n",
      "\n",
      "es = get_es_connection()\n",
      "\n",
      "REINDEX = False\n",
      "\n",
      "# pip install gpt4all\n",
      "# Needs Ubuntu > 22.04 because of glibc\n",
      "\n",
      "if REINDEX:\n",
      "    # Clear the index\n",
      "    es.indices.delete(index=\"eurlex-langchain\", ignore=[400, 404])\n",
      "\n",
      "    start_time = time.time()\n",
      "    vectorstore = \n",
      "--------------------------------------------------------------------------------\n",
      "ASSISTANT\n",
      "ElasticsearchStore(es, \"eurlex-langchain\")\n",
      "    llm = Ollama(OllamaEmbeddings(), vectorstore)\n",
      "    llm.load(\"eurlex-ollama.v1\", \"/mnt/data/eurlex-ollama.v1\")\n",
      "    print(\"Time to load LLM:\", time.time()-start_time)\n",
      "--------------------------------------------------------------------------------\n",
      "USER\n",
      "[INST]\n",
      "The completion you provided resulted in the following errors:\n",
      "your completion:39: error: Too many positional arguments for \"ElasticsearchStore\"  [misc]\n",
      "your completion:39: error: Argument 2 to \"ElasticsearchStore\" has incompatible type \"str\"; expected \"Embeddings | None\"  [arg-type]\n",
      "Found 3 errors in 1 file (checked 1 source file)\n",
      "\n",
      "To fix these errors, use these ground truth signatures as a reference for your next completion:\n",
      "ElasticsearchStore: (index_name: str, *, embedding: Optional[langchain_core.embeddings.Embeddings] = None, es_connection: Optional[ForwardRef('Elasticsearch')] = None, es_url: Optional[str] = None, es_cloud_id: Optional[str] = None, es_user: Optional[str] = None, es_api_key: Optional[str] = None, es_password: Optional[str] = None, vector_query_field: str = 'vector', query_field: str = 'text', distance_strategy: Optional[Literal[<DistanceStrategy.COSINE: 'COSINE'>, <DistanceStrategy.DOT_PRODUCT: 'DOT_PRODUCT'>, <DistanceStrategy.EUCLIDEAN_DISTANCE: 'EUCLIDEAN_DISTANCE'>]] = None, strategy: langchain.vectorstores.elasticsearch.BaseRetrievalStrategy = <langchain.vectorstores.elasticsearch.ApproxRetrievalStrategy object at 0x7f45c4e0c8d0>)\n",
      "\n",
      "Fix, improve and rewrite your completion for the following code:\n",
      "[/INST]\n",
      "from langchain.document_loaders import DirectoryLoader, JSONLoader\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "\n",
      "# Embed and store\n",
      "from langchain.vectorstores.elasticsearch import ElasticsearchStore \n",
      "from langchain.embeddings import GPT4AllEmbeddings\n",
      "from langchain.embeddings import OllamaEmbeddings # We can also try Ollama embeddings\n",
      "\n",
      "from langchain.llms import Ollama\n",
      "from langchain.callbacks.manager import CallbackManager\n",
      "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
      "\n",
      "import os\n",
      "import json\n",
      "from tqdm import tqdm\n",
      "from bs4 import BeautifulSoup\n",
      "import time\n",
      "\n",
      "import torch\n",
      "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "# pip install jq\n",
      "schema = {\n",
      "    'jq_schema': '.text'\n",
      "}\n",
      "\n",
      "es = get_es_connection()\n",
      "\n",
      "REINDEX = False\n",
      "\n",
      "# pip install gpt4all\n",
      "# Needs Ubuntu > 22.04 because of glibc\n",
      "\n",
      "if REINDEX:\n",
      "    # Clear the index\n",
      "    es.indices.delete(index=\"eurlex-langchain\", ignore=[400, 404])\n",
      "\n",
      "    start_time = time.time()\n",
      "    vectorstore = \n",
      "--------------------------------------------------------------------------------\n",
      "ASSISTANT\n",
      "ElasticsearchStore(\"eurlex-langchain\", embedding=OllamaEmbeddings(), es_connection=es)\n",
      "    llm = Ollama(OllamaEmbeddings(), vectorstore)\n",
      "    llm.load(\"eurlex-ollama.v1\", \"/mnt/data/eurlex-ollama.v1\")\n",
      "    print(\"Time to load LLM:\", time.time()-start_time)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for message in llmcoder.messages:\n",
    "    print(message['role'].upper())\n",
    "    # Remove colors from the content of the message\n",
    "    message['content'] = re.sub(r'\\x1b[^m]*m', '', message['content'])\n",
    "    print(message['content'])\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copilot:\n",
    "# from llmcoder import LLMCoder\n",
    "\n",
    "# # Create an LLMCoder with a random seed of 0\n",
    "# llmcoder = LLMCoder(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
