{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcoder import LLMCoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMcoder] SYSTEM: You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\n",
      "You are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\n",
      "Precisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\n",
      "Your code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\n",
      "Your unique capability is to provide completions without altering, repeating, or commenting on the original code.\n",
      "You offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\n",
      "This approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\n",
      "Your response begins with the next characters of the line if the last line of the user's code is incomplete, or the next line if the last line of the user's code is complete.\n",
      "Your application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\n"
     ]
    }
   ],
   "source": [
    "llmcoder = LLMCoder(\n",
    "    analyzers=[\"mypy_analyzer_v1\", \"signature_analyzer_v1\", \"gpt_score_analyzer_v1\"],\n",
    "    max_iter=5,\n",
    "    feedback_variant=\"coworker\",\n",
    "    n_procs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_code = '''from langchain.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embed and store\n",
    "from langchain.vectorstores.elasticsearch import ElasticsearchStore \n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings # We can also try Ollama embeddings\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# pip install jq\n",
    "schema = {\n",
    "    'jq_schema': '.text'\n",
    "}\n",
    "\n",
    "es = get_es_connection()\n",
    "\n",
    "REINDEX = False\n",
    "\n",
    "# pip install gpt4all\n",
    "# Needs Ubuntu > 22.04 because of glibc\n",
    "\n",
    "if REINDEX:\n",
    "    # Clear the index\n",
    "    es.indices.delete(index=\"eurlex-langchain\", ignore=[400, 404])\n",
    "\n",
    "    start_time = time.time()\n",
    "    vectorstore = '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMcoder] SYSTEM: You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\n",
      "You are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\n",
      "Precisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\n",
      "Your code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\n",
      "Your unique capability is to provide completions without altering, repeating, or commenting on the original code.\n",
      "You offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\n",
      "This approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\n",
      "Your response begins with the next characters of the line if the last line of the user's code is incomplete, or the next line if the last line of the user's code is complete.\n",
      "Your application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\n",
      "[LLMcoder] Creating first completion...\n",
      "[LLMcoder] USER: from langchain.document_loaders import DirectoryLoader, JSONLoader\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "\n",
      "# Embed and store\n",
      "from langchain.vectorstores.elasticsearch import ElasticsearchStore \n",
      "from langchain.embeddings import GPT4AllEmbeddings\n",
      "from langchain.embeddings import OllamaEmbeddings # We can also try Ollama embeddings\n",
      "\n",
      "from langchain.llms import Ollama\n",
      "from langchain.callbacks.manager import CallbackManager\n",
      "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
      "\n",
      "import os\n",
      "import json\n",
      "from tqdm import tqdm\n",
      "from bs4 import BeautifulSoup\n",
      "import time\n",
      "\n",
      "import torch\n",
      "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "# pip install jq\n",
      "schema = {\n",
      "    'jq_schema': '.text'\n",
      "}\n",
      "\n",
      "es = get_es_connection()\n",
      "\n",
      "REINDEX = False\n",
      "\n",
      "# pip install gpt4all\n",
      "# Needs Ubuntu > 22.04 because of glibc\n",
      "\n",
      "if REINDEX:\n",
      "    # Clear the index\n",
      "    es.indices.delete(index=\"eurlex-langchain\", ignore=[400, 404])\n",
      "\n",
      "    start_time = time.time()\n",
      "    vectorstore = \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMcoder] Analyzing completion...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running mypy_analyzer_v1...\n"
     ]
    }
   ],
   "source": [
    "completion = llmcoder.complete(initial_code, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_extensions = file_extensions or ['.py']\n"
     ]
    }
   ],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM\n",
      "You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\n",
      "You are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\n",
      "Precisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\n",
      "Your code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\n",
      "Your unique capability is to provide completions without altering, repeating, or commenting on the original code.\n",
      "You offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\n",
      "This approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\n",
      "Your response begins with the next characters of the line if the last line of the user's code is incomplete, or the next line if the last line of the user's code is complete.\n",
      "Your application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\n",
      "--------------------------------------------------------------------------------\n",
      "USER\n",
      "import json\n",
      "import os\n",
      "import random\n",
      "import re\n",
      "\n",
      "import tiktoken\n",
      "from tqdm import tqdm\n",
      "\n",
      "from llmcoder.utils import get_data_dir, get_system_prompt\n",
      "\n",
      "\n",
      "def count_lines(file_path: str) -> int:\n",
      "    \"\"\"\n",
      "    Count the number of lines in a file.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    file_path : str\n",
      "        The path to the file.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    int\n",
      "        The number of lines in the file.\n",
      "    \"\"\"\n",
      "    with open(file_path, 'r') as file:\n",
      "        return sum(1 for _ in file)\n",
      "\n",
      "\n",
      "def get_file_contents(file_paths: list[str]) -> list[str]:\n",
      "    \"\"\"\n",
      "    Get the contents of the given files.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    file_paths : list[str]\n",
      "        A list of paths to the files.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    list[str]\n",
      "        A list of the contents of the files.\n",
      "    \"\"\"\n",
      "    contents = []\n",
      "    for file_path in file_paths:\n",
      "        with open(file_path, 'r') as file:\n",
      "            contents.append(file.read())\n",
      "\n",
      "    return contents\n",
      "\n",
      "\n",
      "def split_file(file_contents: str, min_pos: int = 1, max_pos: int = None) -> tuple[str, str]:\n",
      "    \"\"\"\n",
      "    Split a file at a uniformly random position.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    file_contents : str\n",
      "        The contents of the file.\n",
      "    min_pos : int, optional\n",
      "        The minimum position to split the file at, by default 1\n",
      "    max_pos : int, optional\n",
      "        The maximum position to split the file at, by default None\n",
      "    \"\"\"\n",
      "    if max_pos is None:\n",
      "        max_pos = len(file_contents) - 1\n",
      "\n",
      "    if min_pos > len(file_contents) - 1:\n",
      "        raise ValueError(\"min_pos cannot be greater than the length of the file contents\")\n",
      "\n",
      "    if min_pos < 0:\n",
      "        raise ValueError(\"min_pos cannot be negative\")\n",
      "\n",
      "    if max_pos > len(file_contents) - 1:\n",
      "        raise ValueError(\"max_pos cannot be greater than the length of the file contents\")\n",
      "\n",
      "    if min_pos > max_pos:\n",
      "        raise ValueError(\"min_pos cannot be greater than max_pos\")\n",
      "\n",
      "    split_pos = random.randint(min_pos, max_pos)\n",
      "\n",
      "    return file_contents[:split_pos], file_contents[split_pos:]\n",
      "\n",
      "\n",
      "def sample_files_from_dir(repo_dir: str, n_samples: int = 4, file_extensions: list[str] | None = None) -> list[str]:\n",
      "    \"\"\"\n",
      "    Sample n_samples files from a repository based on the number of lines in each file.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    repo_dir : str\n",
      "        The path to the repository directory.\n",
      "    n_samples : int, optional\n",
      "        The number of files to sample, by default 4\n",
      "    file_extensions : list[str], optional\n",
      "        A list of file extensions to sample from, by default ['.py']\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    list[str]\n",
      "        A list of paths to the sampled files.\n",
      "    \"\"\"\n",
      "    \n",
      "--------------------------------------------------------------------------------\n",
      "ASSISTANT\n",
      "file_extensions = file_extensions or ['.py']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for message in llmcoder.messages:\n",
    "    print(message['role'].upper())\n",
    "    print(message['content'])\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Meta-Loop\n",
    "# N_STEPS = 10\n",
    "\n",
    "# code_steps = [initial_code]\n",
    "\n",
    "# for i in range(N_STEPS):\n",
    "#     completion = llmcoder.complete(code_steps[-1])\n",
    "#     if completion == \"\":\n",
    "#         break\n",
    "#     code_steps.append(code_steps[-1] + completion + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(code_steps[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for message in llmcoder.messages:\n",
    "#     print(message['role'].upper())\n",
    "#     print(message['content'])\n",
    "#     print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(code_steps[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
