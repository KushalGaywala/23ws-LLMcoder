{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcoder import LLMCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmcoder = LLMCoder(\n",
    "    # analyzers=[\"mypy_analyzer_v1\", \"signature_analyzer_v1\"],\n",
    "    analyzers=[\"signature_analyzer_v1\"],\n",
    "    max_iter=5,\n",
    "    feedback_variant=\"coworker\",\n",
    "    n_procs=3,\n",
    "    min_score=5,\n",
    "    backtracking=False, \n",
    "    combining=True,\n",
    "    verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_code = '''\n",
    "        total_generate_candidates = 0\n",
    "\n",
    "        # Get the completions from OpenAI's API\n",
    "        candidates = self.client.chat.completions.create(\n",
    "            messages=conversation.messages,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            n=n)  # type: ignore\n",
    "\n",
    "        total_generate_candidates += len(candidates.choices)\n",
    "\n",
    "        # Count the number of tokens generated\n",
    "        self.n_tokens_generated += sum([len(self.encoder.encode(message.message.content)) for message in candidates.choices])\n",
    "\n",
    "        # Filter out completions that are repetitions of previous mistakes\n",
    "        valid_choices = [completion for completion in candidates.choices if not self._is_bad_completion(completion.message.content)]\n",
    "        print(\"[ME] The number of valid choices is: \", len(valid_choices))\n",
    "        # Remove duplicates\n",
    "        valid_unique_contents = list(set([choice.message.content for choice in valid_choices]))\n",
    "        print(\"[ME] The number of valid unique contents is: \", len(valid_unique_contents))\n",
    "\n",
    "        # If all completions are repetitions of previous mistakes, increase the temperature and the number of choices until we get a valid completion\n",
    "        increased_temperature = temperature\n",
    "        increased_n = n\n",
    "        repetition = 0\n",
    "\n",
    "        while len(valid_unique_contents) < n and repetition < max_retries:\n",
    "            if self.verbose:\n",
    "                print(f\"[LLMcoder] Found {total_generate_candidates - len(valid_choices)} repeated mistakes, {len(valid_choices) - len(valid_unique_contents)} duplicates. Increasing temperature to {increased_temperature:.1f} and number of choices to {increased_n}... [repetition {repetition + 1}/{max_retries}]\")\n",
    "\n",
    "            # Sample new candidates\n",
    "            candidates = self.client.chat.completions.create(\n",
    "                messages=conversation.messages,\n",
    "                model=model,\n",
    "                temperature=increased_temperature,\n",
    "                n=increased_n)  # type: ignore\n",
    "\n",
    "            total_generate_candidates += len(candidates.choices)\n",
    "\n",
    "            # Add the new completions to the list of valid completions\n",
    "            # Filter out completions that are repetitions of previous mistakes\n",
    "            valid_choices += [completion for completion in candidates.choices if not self._is_bad_completion(completion.message.content)]\n",
    "\n",
    "            # Remove duplicates\n",
    "            valid_unique_contents = list(set([choice.message.content for choice in valid_choices]))\n",
    "\n",
    "            increased_temperature = min(max_temperature, increased_temperature + delta_temperature)\n",
    "            increased_n = min(max_n, increased_n * factor_n)\n",
    "            repetition += 1\n",
    "\n",
    "        # If we still do not have valid choices, abort\n",
    "        if len(valid_unique_contents) == 0 and repetition >= max_retries:\n",
    "            if self.verbose:\n",
    "                print(\"[LLMcoder] Coul\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMcoder] Creating first completions...\n",
      "[LLMcoder] Choosing conversation R with score 0\n",
      "[LLMCoder] The number of valid choices is:  3\n",
      "[LLMCoder] The number of valid unique contents is:  3\n",
      "[LLMcoder] Analyzing 3 completions...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[LLMcoder] Have 3 conversations:\n",
      "[LLMcoder] Passing   Score     Prob      Path\n",
      "[LLMcoder] True      0         0.3333    ['R', 0]\n",
      "[LLMcoder] True      0         0.3333    ['R', 1]\n",
      "[LLMcoder] True      0         0.3333    ['R', 2]\n",
      "[LLMcoder] Starting feedback loop...\n",
      "[LLMcoder] Starting feedback iteration 1...\n",
      "[LLMcoder] Choosing conversation R-0 with score 0\n",
      "[LLMCoder] The number of valid choices is:  3\n",
      "[LLMCoder] The number of valid unique contents is:  1\n",
      "[LLMcoder] Found 0 repeated mistakes, 2 duplicates. Increasing temperature to 0.7 and number of choices to 3... [repetition 1/5]\n",
      "[LLMcoder] Found 0 repeated mistakes, 5 duplicates. Increasing temperature to 0.9 and number of choices to 6... [repetition 2/5]\n",
      "[LLMcoder] Found 0 repeated mistakes, 11 duplicates. Increasing temperature to 1.1 and number of choices to 12... [repetition 3/5]\n",
      "[LLMcoder] Found 0 repeated mistakes, 23 duplicates. Increasing temperature to 1.3 and number of choices to 24... [repetition 4/5]\n",
      "[LLMcoder] Found 0 repeated mistakes, 46 duplicates. Increasing temperature to 1.5 and number of choices to 32... [repetition 5/5]\n",
      "[LLMcoder] Analyzing 2 completions...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[LLMCoder] The number of valid choices is:  3\n",
      "[LLMCoder] The number of valid unique contents is:  2\n",
      "[LLMcoder] Found 0 repeated mistakes, 1 duplicates. Increasing temperature to 0.7 and number of choices to 3... [repetition 1/5]\n",
      "[LLMcoder] Found 0 repeated mistakes, 4 duplicates. Increasing temperature to 0.9 and number of choices to 6... [repetition 2/5]\n",
      "[LLMcoder] Found 0 repeated mistakes, 10 duplicates. Increasing temperature to 1.1 and number of choices to 12... [repetition 3/5]\n",
      "[LLMcoder] Analyzing 4 completions...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "WE ARE COMBINING!!!!!!!!\n",
      "WE ARE IN THE COMBINING PROMPT TEMPLATE\n",
      "[LLMCoder] The number of valid choices is:  0\n",
      "[LLMCoder] The number of valid unique contents is:  0\n",
      "[LLMcoder] Found 3 repeated mistakes, 0 duplicates. Increasing temperature to 0.7 and number of choices to 3... [repetition 1/5]\n",
      "[LLMcoder] Found 6 repeated mistakes, 0 duplicates. Increasing temperature to 0.9 and number of choices to 6... [repetition 2/5]\n",
      "[LLMcoder] Found 12 repeated mistakes, 0 duplicates. Increasing temperature to 1.1 and number of choices to 12... [repetition 3/5]\n",
      "[LLMcoder] Analyzing 3 completions...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[LLMcoder] Have 9 conversations:\n",
      "[LLMcoder] Passing   Score     Prob      Path\n",
      "[LLMcoder] True      0         0.1111    ['R', 0, 0]\n",
      "[LLMcoder] True      0         0.1111    ['R', 0, 1]\n",
      "[LLMcoder] True      0         0.1111    ['R', 0, 1, 0]\n",
      "[LLMcoder] True      0         0.1111    ['R', 0, 1, 1]\n",
      "[LLMcoder] True      0         0.1111    ['R', 0, 1, 2]\n",
      "[LLMcoder] True      0         0.1111    ['R', 0, 1, 3]\n",
      "[LLMcoder] True      0         0.1111    ['R', 0, 0]\n",
      "[LLMcoder] True      0         0.1111    ['R', 0, 1]\n",
      "[LLMcoder] True      0         0.1111    ['R', 0, 2]\n",
      "[LLMcoder] Code is correct. Stopping early...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'d not find any valid choice after\", repetition, \"retries. Aborting...\")\\n            return []'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmcoder.complete_with_feedback(code=initial_code, min_score=5, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": llmcoder.system_prompt\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": initial_code\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from llmcoder.utils import get_openai_key\n",
    "\n",
    "client = OpenAI(api_key = get_openai_key())\n",
    "completions = client.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\", temperature=0.7, n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = llmcoder.conversations.pop(keep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llmcoder.conversation.conversation.Conversation at 0x254fc748ed0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.add_message({'role': 'user', 'content': initial_code})\n",
    "conversation.add_message({'role': 'assistant', 'content': completions.choices[0].message.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation2 = conversation.copy().add_message({'role': 'assistant', 'content': completions.choices[1].message.content})\n",
    "llmcoder.conversations.push(conversation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMCoder] Conversation 0 has messages: [{'role': 'system', 'content': \"You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\\nYou are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\\nPrecisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\\nYour code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\\nYour unique capability is to provide completions without altering, repeating, or commenting on the original code.\\nYou offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\\nThis approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\\nYour response begins with the next characters of the line if the last line of the user's code is incomplete, or the next line if the last line of the user's code is complete.\\nYour application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\"}, {'role': 'user', 'content': '\\n        total_generate_candidates = 0\\n\\n        # Get the completions from OpenAI\\'s API\\n        candidates = self.client.chat.completions.create(\\n            messages=conversation.messages,\\n            model=model,\\n            temperature=temperature,\\n            n=n)  # type: ignore\\n\\n        total_generate_candidates += len(candidates.choices)\\n\\n        # Count the number of tokens generated\\n        self.n_tokens_generated += sum([len(self.encoder.encode(message.message.content)) for message in candidates.choices])\\n\\n        # Filter out completions that are repetitions of previous mistakes\\n        valid_choices = [completion for completion in candidates.choices if not self._is_bad_completion(completion.message.content)]\\n        print(\"[ME] The number of valid choices is: \", len(valid_choices))\\n        # Remove duplicates\\n        valid_unique_contents = list(set([choice.message.content for choice in valid_choices]))\\n        print(\"[ME] The number of valid unique contents is: \", len(valid_unique_contents))\\n\\n        # If all completions are repetitions of previous mistakes, increase the temperature and the number of choices until we get a valid completion\\n        increased_temperature = temperature\\n        increased_n = n\\n        repetition = 0\\n\\n        while len(valid_unique_contents) < n and repetition < max_retries:\\n            if self.verbose:\\n                print(f\"[LLMcoder] Found {total_generate_candidates - len(valid_choices)} repeated mistakes, {len(valid_choices) - len(valid_unique_contents)} duplicates. Increasing temperature to {increased_temperature:.1f} and number of choices to {increased_n}... [repetition {repetition + 1}/{max_retries}]\")\\n\\n            # Sample new candidates\\n            candidates = self.client.chat.completions.create(\\n                messages=conversation.messages,\\n                model=model,\\n                temperature=increased_temperature,\\n                n=increased_n)  # type: ignore\\n\\n            total_generate_candidates += len(candidates.choices)\\n\\n            # Add the new completions to the list of valid completions\\n            # Filter out completions that are repetitions of previous mistakes\\n            valid_choices += [completion for completion in candidates.choices if not self._is_bad_completion(completion.message.content)]\\n\\n            # Remove duplicates\\n            valid_unique_contents = list(set([choice.message.content for choice in valid_choices]))\\n\\n            increased_temperature = min(max_temperature, increased_temperature + delta_temperature)\\n            increased_n = min(max_n, increased_n * factor_n)\\n            repetition += 1\\n\\n        # If we still do not have valid choices, abort\\n        if len(valid_unique_contents) == 0 and repetition >= max_retries:\\n            if self.verbose:\\n                print(\"[LLMcoder] Coul\\n'}, {'role': 'assistant', 'content': 'd not find any valid completions after the maximum retries. Aborting process.\")'}]\n",
      "[LLMCoder] Conversation 1 has messages: [{'role': 'system', 'content': \"You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\\nYou are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\\nPrecisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\\nYour code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\\nYour unique capability is to provide completions without altering, repeating, or commenting on the original code.\\nYou offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\\nThis approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\\nYour response begins with the next characters of the line if the last line of the user's code is incomplete, or the next line if the last line of the user's code is complete.\\nYour application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\"}, {'role': 'user', 'content': '\\n        total_generate_candidates = 0\\n\\n        # Get the completions from OpenAI\\'s API\\n        candidates = self.client.chat.completions.create(\\n            messages=conversation.messages,\\n            model=model,\\n            temperature=temperature,\\n            n=n)  # type: ignore\\n\\n        total_generate_candidates += len(candidates.choices)\\n\\n        # Count the number of tokens generated\\n        self.n_tokens_generated += sum([len(self.encoder.encode(message.message.content)) for message in candidates.choices])\\n\\n        # Filter out completions that are repetitions of previous mistakes\\n        valid_choices = [completion for completion in candidates.choices if not self._is_bad_completion(completion.message.content)]\\n        print(\"[ME] The number of valid choices is: \", len(valid_choices))\\n        # Remove duplicates\\n        valid_unique_contents = list(set([choice.message.content for choice in valid_choices]))\\n        print(\"[ME] The number of valid unique contents is: \", len(valid_unique_contents))\\n\\n        # If all completions are repetitions of previous mistakes, increase the temperature and the number of choices until we get a valid completion\\n        increased_temperature = temperature\\n        increased_n = n\\n        repetition = 0\\n\\n        while len(valid_unique_contents) < n and repetition < max_retries:\\n            if self.verbose:\\n                print(f\"[LLMcoder] Found {total_generate_candidates - len(valid_choices)} repeated mistakes, {len(valid_choices) - len(valid_unique_contents)} duplicates. Increasing temperature to {increased_temperature:.1f} and number of choices to {increased_n}... [repetition {repetition + 1}/{max_retries}]\")\\n\\n            # Sample new candidates\\n            candidates = self.client.chat.completions.create(\\n                messages=conversation.messages,\\n                model=model,\\n                temperature=increased_temperature,\\n                n=increased_n)  # type: ignore\\n\\n            total_generate_candidates += len(candidates.choices)\\n\\n            # Add the new completions to the list of valid completions\\n            # Filter out completions that are repetitions of previous mistakes\\n            valid_choices += [completion for completion in candidates.choices if not self._is_bad_completion(completion.message.content)]\\n\\n            # Remove duplicates\\n            valid_unique_contents = list(set([choice.message.content for choice in valid_choices]))\\n\\n            increased_temperature = min(max_temperature, increased_temperature + delta_temperature)\\n            increased_n = min(max_n, increased_n * factor_n)\\n            repetition += 1\\n\\n        # If we still do not have valid choices, abort\\n        if len(valid_unique_contents) == 0 and repetition >= max_retries:\\n            if self.verbose:\\n                print(\"[LLMcoder] Coul\\n'}, {'role': 'assistant', 'content': 'd not find any valid completions after the maximum retries. Aborting process.\")'}, {'role': 'assistant', 'content': 'd not find valid completions after maximum retries. Aborting the process.\"'}]\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate([conversation, conversation2]):\n",
    "    print(f'[LLMCoder] Conversation {i} has messages: {c.messages}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(conversation.analyses) > 0 and len(conversation2.analyses) > 0:\n",
    "    combining_prompt = llmcoder._combining_prompt_template(conversation, conversation2,\n",
    "                                                                # If there is feedback available from previous analyses, add it to the prompt\n",
    "                                                                [str(result['message']) for result in conversation.analyses[-1].values()\n",
    "                                                                    if (not result['pass'] or result['type'] == \"info\")],\n",
    "                                                                [str(result['message']) for result in conversation2.analyses[-1].values()\n",
    "                                                                    if (not result['pass'] or result['type'] == \"info\")])\n",
    "elif len(conversation.analyses) > 0:\n",
    "    combining_prompt = llmcoder._combining_prompt_template(conversation, conversation2,\n",
    "                                                        [str(result['message']) for result in conversation.analyses[-1].values()\n",
    "                                                            if (not result['pass'] or result['type'] == \"info\")],\n",
    "                                                        None)\n",
    "elif len(conversation2.analyses) > 0:\n",
    "    combining_prompt = llmcoder._combining_prompt_template(conversation, conversation2,\n",
    "                                                        None,\n",
    "                                                        [str(result['message']) for result in conversation2.analyses[-1].values()\n",
    "                                                            if (not result['pass'] or result['type'] == \"info\")])\n",
    "else:\n",
    "    combining_prompt = llmcoder._combining_prompt_template(conversation, conversation2, None, None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMCoder] The number of valid choices is:  3\n",
      "[LLMCoder] The number of valid unique contents is:  3\n",
      "WE HAVE VALID CHOICES!!!!!!!!\n",
      "[LLMcoder] Analyzing 3 completions...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[LLMcoder] Analyzing code in coworker mode...\n",
      "[LLMcoder] Running signature_analyzer_v1...\n",
      "[Signatures] No problematic functions or classes found in the context.\n",
      "[Signatures] No problematic functions or classes found in the context.\n"
     ]
    }
   ],
   "source": [
    "conversation.add_message({'role': 'user', 'content': combining_prompt})\n",
    "llmcoder.conversations.push(conversation)\n",
    "llmcoder._get_completions_for(conversation, llmcoder.model_feedback, temperature=0.7, n=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
