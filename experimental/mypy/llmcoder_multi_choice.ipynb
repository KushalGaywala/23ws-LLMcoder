{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcoder import LLMCoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/CodeBERT-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "llmcoder = LLMCoder(analyzers=[\"mypy_analyzer_v1\", \"signature_analyzer_v1\"], max_iter=3, feedback_variant=\"coworker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import openai\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from llmcoder.analyze.factory import AnalyzerFactory\n",
    "from llmcoder.utils import get_conversations_dir, get_openai_key, get_system_prompt, get_system_prompt_dir\n",
    "\n",
    "\n",
    "class LLMCoder:\n",
    "    def __init__(self,\n",
    "                 analyzers: list[str] = None,\n",
    "                 model_first: str = \"ft:gpt-3.5-turbo-1106:personal::8LCi9Q0d\",\n",
    "                 model_feedback: str = \"gpt-3.5-turbo\",\n",
    "                 feedback_variant: str = \"separate\",\n",
    "                 system_prompt: str | None = None,\n",
    "                 max_iter: int = 10,\n",
    "                 log_conversation: bool = True,\n",
    "                 device: str | torch.device | None = None):\n",
    "        \"\"\"\n",
    "        Initialize the LLMCoder\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        analyzers : list[str], optional\n",
    "            The list of analyzers to use, by default []\n",
    "        model_first : str, optional\n",
    "            The model to use for the first completion, by default \"ft:gpt-3.5-turbo-1106:personal::8LCi9Q0d\"\n",
    "        model_feedback : str, optional\n",
    "            The model to use for the feedback loop, by default \"gpt-3.5-turbo\"\n",
    "        feedback_variant : str, optional\n",
    "            The feedback variant to use, by default \"separate\"\n",
    "        system_prompt : str, optional\n",
    "            The system prompt to use, by default the one used for preprocessing and fine-tuning\n",
    "        max_iter : int, optional\n",
    "            The maximum number of iterations to run the feedback loop, by default 10\n",
    "        log_conversation : bool, optional\n",
    "            Whether to log the conversation, by default False\n",
    "        device : str | torch.device | None, optional\n",
    "            The device to use for the scoring model, by default the first available GPU or CPU\n",
    "        \"\"\"\n",
    "        # Check for invalid feedback variants\n",
    "        if feedback_variant not in [\"separate\", \"coworker\"]:\n",
    "            raise ValueError(\"Inavlid feedback method\")\n",
    "        self.feedback_variant = feedback_variant\n",
    "\n",
    "        # Remember the models\n",
    "        self.model_first = model_first\n",
    "        self.model_feedback = model_feedback\n",
    "\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating first completion...\n",
      "Considering 14 completions\n",
      "Choosing message 0 with score 0.2659682631492615\n",
      "Starting feedback loop...\n",
      "Starting feedback iteration 1...\n",
      "Analyzing code in a coworker mode...\n",
      "Running mypy_analyzer_v1...\n",
      "No missing stubs found.\n",
      "/tmp/tmp9jvxnk5_.py:7: error: Skipping analyzing \"transformers\": module is installed, but missing library stubs or py.typed marker  [import-untyped]\n",
      "/tmp/tmp9jvxnk5_.py:7: note: See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-imports\n",
      "/tmp/tmp9jvxnk5_.py:15: error: Incompatible default for argument \"analyzers\" (default has type \"None\", argument has type \"list[str]\")  [assignment]\n",
      "/tmp/tmp9jvxnk5_.py:15: note: PEP 484 prohibits implicit Optional. Accordingly, mypy has changed its default to no_implicit_optional=True\n",
      "/tmp/tmp9jvxnk5_.py:15: note: Use https://github.com/hauntsaninja/no_implicit_optional to automatically upgrade your codebase\n",
      "Found 2 errors in 1 file (checked 1 source file)\n",
      "\n",
      "Running signature_analyzer_v1...\n",
      "Using context from previous analyzers: ['mypy_analyzer_v1']\n",
      "No problematic functions or classes found in the context.\n",
      "1 / 1 analyzers passed\n"
     ]
    }
   ],
   "source": [
    "result = llmcoder.complete(code, temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Set the device\n",
      "        if device is None:\n",
      "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "        elif isinstance(device, str):\n",
      "            self.device = torch.device(device)\n",
      "        else:\n",
      "            self.device = device\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM\n",
      "You are AutocompleteGPT, a useful AI autocomplete tool that provides code completions based on the user's code.\n",
      "You are a precision-focused tool for code autocompletion, adept in languages like Python, JavaScript, C++, and SQL.\n",
      "Precisely continue the code from the point of interruption and do not repeat or modify the original code, even if it is incorrect or the code interrupts in the middle of a line.\n",
      "Your code is well documented with comments and annotations, and you should provide a clear explanation of the code's purpose in your code completion.\n",
      "Your unique capability is to provide completions without altering, repeating, or commenting on the original code.\n",
      "You offer only the necessary code to complete the snippet, ensuring the response is exclusively code, with no additional comments, explanations, or annotations.\n",
      "This approach makes you an ideal assistant for users seeking straightforward and efficient code extensions, enhancing their work with accurate, logic-driven completions while maintaining the integrity and simplicity of the original input.\n",
      "Your response begins with the next characters of the line if the last line of the user's code is incomplete, or the next line if the last line of the user's code is complete.\n",
      "Your application is a VSCode extension like GitHub Copilot, which provides seamless code completions based on the user's code at the point of interruption.\n",
      "--------------------------------------------------------------------------------\n",
      "USER\n",
      "import json\n",
      "import os\n",
      "from datetime import datetime\n",
      "\n",
      "import openai\n",
      "import torch\n",
      "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
      "\n",
      "from llmcoder.analyze.factory import AnalyzerFactory\n",
      "from llmcoder.utils import get_conversations_dir, get_openai_key, get_system_prompt, get_system_prompt_dir\n",
      "\n",
      "\n",
      "class LLMCoder:\n",
      "    def __init__(self,\n",
      "                 analyzers: list[str] = None,\n",
      "                 model_first: str = \"ft:gpt-3.5-turbo-1106:personal::8LCi9Q0d\",\n",
      "                 model_feedback: str = \"gpt-3.5-turbo\",\n",
      "                 feedback_variant: str = \"separate\",\n",
      "                 system_prompt: str | None = None,\n",
      "                 max_iter: int = 10,\n",
      "                 log_conversation: bool = True,\n",
      "                 device: str | torch.device | None = None):\n",
      "        \"\"\"\n",
      "        Initialize the LLMCoder\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        analyzers : list[str], optional\n",
      "            The list of analyzers to use, by default []\n",
      "        model_first : str, optional\n",
      "            The model to use for the first completion, by default \"ft:gpt-3.5-turbo-1106:personal::8LCi9Q0d\"\n",
      "        model_feedback : str, optional\n",
      "            The model to use for the feedback loop, by default \"gpt-3.5-turbo\"\n",
      "        feedback_variant : str, optional\n",
      "            The feedback variant to use, by default \"separate\"\n",
      "        system_prompt : str, optional\n",
      "            The system prompt to use, by default the one used for preprocessing and fine-tuning\n",
      "        max_iter : int, optional\n",
      "            The maximum number of iterations to run the feedback loop, by default 10\n",
      "        log_conversation : bool, optional\n",
      "            Whether to log the conversation, by default False\n",
      "        device : str | torch.device | None, optional\n",
      "            The device to use for the scoring model, by default the first available GPU or CPU\n",
      "        \"\"\"\n",
      "        # Check for invalid feedback variants\n",
      "        if feedback_variant not in [\"separate\", \"coworker\"]:\n",
      "            raise ValueError(\"Inavlid feedback method\")\n",
      "        self.feedback_variant = feedback_variant\n",
      "\n",
      "        # Remember the models\n",
      "        self.model_first = model_first\n",
      "        self.model_feedback = model_feedback\n",
      "\n",
      "        \n",
      "--------------------------------------------------------------------------------\n",
      "ASSISTANT\n",
      "# Set the device\n",
      "        if device is None:\n",
      "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "        elif isinstance(device, str):\n",
      "            self.device = torch.device(device)\n",
      "        else:\n",
      "            self.device = device\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for message in llmcoder.messages:\n",
    "    print(message['role'].upper())\n",
    "    # Remove colors from the content of the message\n",
    "    message['content'] = re.sub(r'\\x1b[^m]*m', '', message['content'])\n",
    "    print(message['content'])\n",
    "    print('-' * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
