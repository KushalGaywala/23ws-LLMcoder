[
    {
        "role": "system",
        "content": "\n# Instructions:\nYou are an autocomplete system for Python. Generate or complete Python code based on the user's description or the provided code snippet.\n\nThe user may\n1. Describe code, either directly or in a Python comment. In this case, you should implement the described code.\n2. Write incomplete code. In this case, you should complete the code from the last cursor position.\n3. If the last character is a newline or tab, come up with one or more possible next lines of code.\n\n\n## Example 1:\n\n[INPUT]\n# Function that adds two numbers\ndef\n[/INPUT]\n\n[ANSWER]\n```python\n add_numbers(a, b):\n    return a + b\n```\n[/ANSWER]\n\n\n## Example 2:\n\n[INPUT]\n```python\ndef factorial(n):\n    # Computes the fact\n```\n[/INPUT]\n\n[ANSWER]\n```python\n    # Computes the factorial of a number\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n```\n[/ANSWER]\n\n\n## Example 3:\n\n[INPUT]\nI need a function that checks if a number is prime\n[/INPUT]\n\n[ANSWER]\n```python\ndef is_prime(number):\n    if number > 1:\n        for i in range(2, int(number**0.5)+1):\n            if (number % i) == 0:\n                return False\n        return True\n    else:\n        return False\n```\n[/ANSWER]\n\n\n# Rules:\n- Your entire response must be valid Python code.\n- Avoid evasive or incomplete answers.\n- Any comments or notes must be made in Python code block, i.e., ```python ... ```.\n- You must not add any additional comments or notes outside of the ```python ... ``` blocks. Your entire response must only consist of one Python code block.\n- You must end your response after ending the Python code block with \"```\".\n"
    },
    {
        "role": "user",
        "content": "import numpy as np\nimport pandas as pd\nimport transformers\nfrom scipy.special import softmax as scipy_softmax\nfrom torch.nn.functional import softmax as torch_softmax\nfrom tqdm import tqdm\n\nfrom .data.preprocessing import combine_premise_hypothesis, construct_hypothesis\n\n\ndef predict_heads(model: transformers.PreTrainedModel, tokenizer: transformers.PreTrainedTokenizer, input_text: str | list[str]) -> np.ndarray:\n    \"\"\"\n    Predicts the entailment of the input text.\n\n    Parameters\n    ----------\n    model : transformers.PreTrainedModel\n        The model to use.\n    tokenizer : transformers.PreTrainedTokenizer\n        The tokenizer to use.\n    input_text : str\n        The input text.\n\n    Returns\n    -------\n    outputs : np.ndarray, shape (n_samples, 3)\n        The entailment probabilities, ordered as follows: contradiction, neutral, entailment.\n    \"\"\"\n    # Convert the input text to a list if it is a string\n    if isinstance(input_text, str):\n        input_text = [input_text]\n\n    # If the sentence and entity are other types of iterables, convert them to lists\n    if isinstance(input_text, pd.Series):\n        input_text = input_text.tolist()\n\n    "
    },
    {
        "role": "assistant",
        "content": "```python\n    # Tokenize the input text\n    encoded_inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors='pt')\n\n    # Forward pass through the model\n    with torch.no_grad():\n        outputs = model(**encoded_inputs).logits\n\n    # Apply softmax to get the probabilities\n    probabilities = torch_softmax(outputs, dim=1).numpy()\n\n    return probabilities\n```\n```"
    },
    {
        "role": "comment",
        "content": "\n+ Good completion \n? Are the functions called correctly?\n? Are the types and arguments correct?\n- Adds an additional ``` to the end of the code block\n\nGround Truth:\n```python\n    # Compute the outputs of the model\n    input_ids, attention_mask = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=True, padding=True).values()\n    outputs = model(input_ids.to(model.device), attention_mask=attention_mask.to(model.device))\n\n    # Return the probabilities of the model\n    return torch_softmax(outputs[0], dim=1).detach().cpu().numpy()\n\n```\n"
    }
]