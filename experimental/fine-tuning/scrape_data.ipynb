{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "GITHUB_API = \"https://api.github.com\"\n",
    "\n",
    "with open(\"../../gh_access.txt\", \"r\") as file:\n",
    "    ACCESS_TOKEN = file.read().strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repos_with_query(query, num_repos):\n",
    "    \"\"\"\n",
    "    Fetch repositories using a specific GitHub API query.\n",
    "    \"\"\"\n",
    "    headers = {'Authorization': f'token {ACCESS_TOKEN}'}\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'sort': 'stars',\n",
    "        'order': 'desc',\n",
    "        'per_page': num_repos\n",
    "    }\n",
    "    response = requests.get(f\"{GITHUB_API}/search/repositories\", headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['items']\n",
    "    else:\n",
    "        print(f\"Failed to fetch repositories: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def get_popular_repos(num_repos_per_query=1):\n",
    "    \"\"\"\n",
    "    Fetch popular repositories using different queries.\n",
    "    \"\"\"\n",
    "    repos = []\n",
    "    queries = [\n",
    "        'language:python',\n",
    "        'django in:name,description',\n",
    "        'flask in:name,description',\n",
    "        'data-science in:name,description',\n",
    "        'machine-learning in:name,description'\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        repos.extend(get_repos_with_query(query, num_repos_per_query))\n",
    "\n",
    "    # Removing potential duplicates\n",
    "    unique_repos = {repo['id']: repo for repo in repos}.values()\n",
    "    \n",
    "    return list(unique_repos)\n",
    "\n",
    "def clone_repo(repo_url, output_dir):\n",
    "    \"\"\"\n",
    "    Clone a repository into a specified directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    subprocess.run([\"git\", \"clone\", repo_url, output_dir])\n",
    "\n",
    "\n",
    "def extract_python_files(repo_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Extract all Python files from a repository and place them in a flat structure in the output directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for root, dirs, files in os.walk(repo_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                shutil.copy(file_path, os.path.join(output_dir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rationale\n",
    "- Medium number of repositories\n",
    "- Diverse uses of python\n",
    "- Popular repositories reflect current python trends\n",
    "- Popular packages are programmed by professionals and have good code quality\n",
    "- Personal repositories reflect personal use cases of python\n",
    "- Liked repositories reflect personal interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 Python repositories by stars\n",
    "all_popular_repos = get_popular_repos(3)\n",
    "popular_repos = all_popular_repos[:10]\n",
    "backup_repos = all_popular_repos[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(popular_repos), len(backup_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 5 of the most widely used Python packages\n",
    "popular_package_repos_urls = [\n",
    "    \"https://github.com/numpy/numpy\",\n",
    "    \"https://github.com/pandas-dev/pandas\",\n",
    "    \"https://github.com/matplotlib/matplotlib\",\n",
    "    \"https://github.com/scikit-learn/scikit-learn\",\n",
    "    \"https://github.com/python-pillow/Pillow\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some personal repositories\n",
    "personal_repos_urls = [\n",
    "    \"https://github.com/psaegert/pmtrendviz\",\n",
    "    \"https://github.com/psaegert/nli-nec\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some repositories that I like\n",
    "liked_repos_urls = [\n",
    "    \"https://github.com/graphdeco-inria/gaussian-splatting\",\n",
    "    \"https://github.com/lllyasviel/ControlNet\",\n",
    "    \"https://github.com/maltfield/awesome-lemmy-instances\",\n",
    "    \"https://github.com/Aleph-Alpha/aleph-alpha-client\",\n",
    "    \"https://github.com/MaartenGr/BERTopic\",\n",
    "    \"https://github.com/MilesCranmer/PySR\",\n",
    "    \"https://github.com/AUTOMATIC1111/stable-diffusion-webui\",\n",
    "    \"https://github.com/microsoft/Codex-CLI\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the lists into a list of tuples of (repo_url, repo_name)\n",
    "repos_urls = popular_package_repos_urls + personal_repos_urls + liked_repos_urls\n",
    "repos_names = [url.split(\"/\")[-1] for url in repos_urls]\n",
    "repos = list(zip(repos_urls, repos_names))\n",
    "\n",
    "# Add the popular repositories to the list\n",
    "repos.extend([(repo['html_url'], repo['name']) for repo in popular_repos])\n",
    "\n",
    "# Lastly, add the backup repositories (they will be skipped in case the goal of 25 is reached)\n",
    "repos.extend([(repo['html_url'], repo['name']) for repo in backup_repos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not len(set(repos)) == len(repos):\n",
    "    # Find the duplicate\n",
    "    seen = set()\n",
    "    for repo in repos:\n",
    "        if repo in seen:\n",
    "            print(repo)\n",
    "        else:\n",
    "            seen.add(repo)\n",
    "    raise Exception(\"Duplicate repositories found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_dir = \"scraped_repos\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]Cloning into 'cloned_repos/numpy'...\n",
      "  3%|▎         | 1/30 [00:21<10:22, 21.46s/it]Cloning into 'cloned_repos/pandas'...\n",
      "  7%|▋         | 2/30 [01:09<17:13, 36.92s/it]Cloning into 'cloned_repos/matplotlib'...\n",
      " 10%|█         | 3/30 [03:01<32:01, 71.18s/it]Cloning into 'cloned_repos/scikit-learn'...\n",
      " 13%|█▎        | 4/30 [03:25<22:52, 52.80s/it]Cloning into 'cloned_repos/Pillow'...\n",
      " 17%|█▋        | 5/30 [03:51<17:58, 43.16s/it]Cloning into 'cloned_repos/pmtrendviz'...\n",
      " 20%|██        | 6/30 [03:52<11:28, 28.68s/it]Cloning into 'cloned_repos/nli-nec'...\n",
      " 23%|██▎       | 7/30 [04:07<09:16, 24.19s/it]Cloning into 'cloned_repos/gaussian-splatting'...\n",
      " 27%|██▋       | 8/30 [04:09<06:16, 17.11s/it]Cloning into 'cloned_repos/ControlNet'...\n",
      " 30%|███       | 9/30 [04:32<06:40, 19.05s/it]Cloning into 'cloned_repos/awesome-lemmy-instances'...\n",
      " 33%|███▎      | 10/30 [04:36<04:49, 14.45s/it]Cloning into 'cloned_repos/aleph-alpha-client'...\n",
      " 37%|███▋      | 11/30 [04:37<03:16, 10.37s/it]Cloning into 'cloned_repos/BERTopic'...\n",
      " 40%|████      | 12/30 [04:40<02:25,  8.11s/it]Cloning into 'cloned_repos/PySR'...\n",
      " 43%|████▎     | 13/30 [04:42<01:42,  6.01s/it]Cloning into 'cloned_repos/stable-diffusion-webui'...\n",
      " 47%|████▋     | 14/30 [04:46<01:30,  5.67s/it]Cloning into 'cloned_repos/Codex-CLI'...\n",
      " 50%|█████     | 15/30 [04:55<01:38,  6.60s/it]Cloning into 'cloned_repos/hydra'...\n",
      " 53%|█████▎    | 16/30 [04:56<01:07,  4.79s/it]Cloning into 'cloned_repos/unet_keras'...\n",
      " 57%|█████▋    | 17/30 [05:00<00:59,  4.61s/it]Cloning into 'cloned_repos/ml_class'...\n",
      " 60%|██████    | 18/30 [05:01<00:41,  3.48s/it]Cloning into 'cloned_repos/realworld'...\n",
      " 63%|██████▎   | 19/30 [05:06<00:43,  3.99s/it]Cloning into 'cloned_repos/django'...\n",
      " 67%|██████▋   | 20/30 [05:41<02:14, 13.45s/it]Cloning into 'cloned_repos/django-rest-framework'...\n",
      " 70%|███████   | 21/30 [05:49<01:46, 11.78s/it]Cloning into 'cloned_repos/flask'...\n",
      " 73%|███████▎  | 22/30 [05:51<01:10,  8.84s/it]Cloning into 'cloned_repos/httpbin'...\n",
      " 77%|███████▋  | 23/30 [05:52<00:44,  6.40s/it]Cloning into 'cloned_repos/awesome-flask'...\n",
      " 80%|████████  | 24/30 [05:53<00:27,  4.63s/it]Cloning into 'cloned_repos/PythonDataScienceHandbook'...\n",
      " 83%|████████▎ | 25/30 [06:00<00:27,  5.42s/it]Cloning into 'cloned_repos/data-science-ipython-notebooks'...\n",
      " 87%|████████▋ | 26/30 [06:09<00:26,  6.57s/it]Cloning into 'cloned_repos/applied-ml'...\n",
      " 90%|█████████ | 27/30 [06:10<00:14,  4.79s/it]Cloning into 'cloned_repos/tensorflow'...\n",
      "Updating files: 100% (31172/31172), done.\n",
      "100%|██████████| 30/30 [08:32<00:00, 17.07s/it]\n"
     ]
    }
   ],
   "source": [
    "for repo_url, repo_name in tqdm(repos):\n",
    "    clone_repo_dir = f\"cloned_repos/{repo_name}\"\n",
    "    output_repo_dir = os.path.join(base_output_dir, repo_name)\n",
    "\n",
    "    # Check if the repository has already been cloned\n",
    "    if os.path.exists(os.path.join(base_output_dir, repo_name)):\n",
    "        continue\n",
    "\n",
    "    if len(os.listdir(\"scraped_repos\")) >= 25:\n",
    "        continue\n",
    "    \n",
    "    clone_repo(repo_url, clone_repo_dir)\n",
    "    extract_python_files(clone_repo_dir, output_repo_dir)\n",
    "\n",
    "    # If the repository is empty, remove the directory\n",
    "    if len(os.listdir(output_repo_dir)) == 0:\n",
    "        shutil.rmtree(output_repo_dir)\n",
    "        continue\n",
    "    \n",
    "    with open(\"repositories.txt\", \"a\") as file:\n",
    "        file.write(repo_url + \"\\n\")\n",
    "\n",
    "    # Optionally, remove the cloned repo directory to save space\n",
    "    shutil.rmtree(clone_repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'cloned_repos' directory\n",
    "shutil.rmtree(\"cloned_repos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the respositories.txt file and check that all repositories have been scraped\n",
    "with open(\"repositories.txt\", \"r\") as file:\n",
    "    urls = file.read()\n",
    "    # Remove the last newline character\n",
    "    urls = urls[:-1]\n",
    "    urls = urls.split(\"\\n\")\n",
    "\n",
    "repo_names = [url.split(\"/\")[-1] for url in urls]\n",
    "folder_names = os.listdir(base_output_dir)\n",
    "\n",
    "for repo_name in repo_names:\n",
    "    if repo_name not in folder_names:\n",
    "        print(f\"Missing repository: {repo_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
