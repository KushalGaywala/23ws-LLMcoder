{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up OpenAI and start with a system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES_DIR = 'examples'\n",
    "os.makedirs(EXAMPLES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=open('../key.txt', 'r').read().strip('\\n'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "# Instructions:\n",
    "You are an autocomplete system for Python. Generate or complete Python code based on the user's description or the provided code snippet.\n",
    "\n",
    "The user may\n",
    "1. Describe code, either directly or in a Python comment. In this case, you should implement the described code.\n",
    "2. Write incomplete code. In this case, you should complete the code from the last cursor position.\n",
    "3. If the last character is a newline or tab, come up with one or more possible next lines of code.\n",
    "\n",
    "\n",
    "## Example 1:\n",
    "\n",
    "[INPUT]\n",
    "# Function that adds two numbers\n",
    "def\n",
    "[/INPUT]\n",
    "\n",
    "[ANSWER]\n",
    "```python\n",
    " add_numbers(a, b):\n",
    "    return a + b\n",
    "```\n",
    "[/ANSWER]\n",
    "\n",
    "\n",
    "## Example 2:\n",
    "\n",
    "[INPUT]\n",
    "```python\n",
    "def factorial(n):\n",
    "    # Computes the fact\n",
    "```\n",
    "[/INPUT]\n",
    "\n",
    "[ANSWER]\n",
    "```python\n",
    "    # Computes the factorial of a number\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)\n",
    "```\n",
    "[/ANSWER]\n",
    "\n",
    "\n",
    "## Example 3:\n",
    "\n",
    "[INPUT]\n",
    "I need a function that checks if a number is prime\n",
    "[/INPUT]\n",
    "\n",
    "[ANSWER]\n",
    "```python\n",
    "def is_prime(number):\n",
    "    if number > 1:\n",
    "        for i in range(2, int(number**0.5)+1):\n",
    "            if (number % i) == 0:\n",
    "                return False\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "```\n",
    "[/ANSWER]\n",
    "\n",
    "\n",
    "# Rules:\n",
    "- Your entire response must be valid Python code.\n",
    "- Avoid evasive or incomplete answers.\n",
    "- Any comments or notes must be made in Python code block, i.e., ```python ... ```.\n",
    "- You must not add any additional comments or notes outside of the ```python ... ``` blocks. Your entire response must only consist of one Python code block.\n",
    "- You must end your response after ending the Python code block with \"```\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_message(messages: list[str], role: str, message: str | None = None) -> list[str]:\n",
    "    # If the user is the assistant, generate a response\n",
    "    if role == \"assistant\" and message is None:\n",
    "        chat_completion = client.chat.completions.create(messages=messages, model=\"gpt-3.5-turbo\")\n",
    "        message = chat_completion.choices[0].message.content\n",
    "\n",
    "    # Add the message to the messages list\n",
    "    messages.append({\n",
    "        \"role\": role,\n",
    "        \"content\": message,\n",
    "    })\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a response to an example prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = '''import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from scipy.special import softmax as scipy_softmax\n",
    "from torch.nn.functional import softmax as torch_softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "from .data.preprocessing import combine_premise_hypothesis, construct_hypothesis\n",
    "\n",
    "\n",
    "def predict_heads(model: transformers.PreTrainedModel, tokenizer: transformers.PreTrainedTokenizer, input_text: str | list[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predicts the entailment of the input text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : transformers.PreTrainedModel\n",
    "        The model to use.\n",
    "    tokenizer : transformers.PreTrainedTokenizer\n",
    "        The tokenizer to use.\n",
    "    input_text : str\n",
    "        The input text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    outputs : np.ndarray, shape (n_samples, 3)\n",
    "        The entailment probabilities, ordered as follows: contradiction, neutral, entailment.\n",
    "    \"\"\"\n",
    "    # Convert the input text to a list if it is a string\n",
    "    if isinstance(input_text, str):\n",
    "        input_text = [input_text]\n",
    "\n",
    "    # If the sentence and entity are other types of iterables, convert them to lists\n",
    "    if isinstance(input_text, pd.Series):\n",
    "        input_text = input_text.tolist()\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = add_message(messages, \"system\", system_prompt)\n",
    "messages = add_message(messages, \"user\", user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = add_message(messages, \"assistant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the output and store the conversation and a comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "    # Tokenize the input text\n",
      "    encoded_inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors='pt')\n",
      "\n",
      "    # Forward pass through the model\n",
      "    with torch.no_grad():\n",
      "        outputs = model(**encoded_inputs).logits\n",
      "\n",
      "    # Apply softmax to get the probabilities\n",
      "    probabilities = torch_softmax(outputs, dim=1).numpy()\n",
      "\n",
      "    return probabilities\n",
      "```\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(messages[-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = \"\"\"\n",
    "+ Good completion \n",
    "? Are the functions called correctly?\n",
    "? Are the types and arguments correct?\n",
    "- Adds an additional ``` to the end of the code block\n",
    "\n",
    "Ground Truth:\n",
    "```python\n",
    "    # Compute the outputs of the model\n",
    "    input_ids, attention_mask = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=True, padding=True).values()\n",
    "    outputs = model(input_ids.to(model.device), attention_mask=attention_mask.to(model.device))\n",
    "\n",
    "    # Return the probabilities of the model\n",
    "    return torch_softmax(outputs[0], dim=1).detach().cpu().numpy()\n",
    "\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "title = \"Complete predict_heads function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any previous comments\n",
    "messages = [message for message in messages if message['role'] != 'comment']\n",
    "\n",
    "# Add the comment to the messages list\n",
    "messages.append({\n",
    "    \"role\": \"comment\",\n",
    "    \"content\": comment,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the messages to a file\n",
    "if not os.path.exists(os.path.join(EXAMPLES_DIR, f\"{title}.json\")):\n",
    "    with open(os.path.join(EXAMPLES_DIR, f\"{title}.json\"), \"w\") as f:\n",
    "        json.dump(messages, f, indent=4)\n",
    "else:\n",
    "    print(f\"File {title}.json already exists. Skipping.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
